{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# An approach towards Depth Estimation with CNN\n",
    "\n",
    "**Author:** [Victor Basu](https://www.linkedin.com/in/victor-basu-520958147)<br>\n",
    "**Date created:** 2021/08/09<br>\n",
    "**Last modified:** 2021/08/10<br>\n",
    "**Description:** Implement an depth estimation model with CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Title: An approach towards Depth Estimation with CNN.\n",
    "\n",
    "Author: [Victor Basu](https://www.linkedin.com/in/victor-basu-520958147)\n",
    "\n",
    "Date created: 2021/08/08\n",
    "\n",
    "Last modified: 2021/08/09\n",
    "\n",
    "Description: Implement an depth estimation model with CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**Depth Estimation** is a crucial step towards inferring scene geometry from 2D images.\n",
    "The goal in monocular Depth Estimation is to predict the depth value of each pixel, given\n",
    "only a single RGB image as input.\n",
    "\n",
    "This is an approach to build a depth estimation model with CNN and basic loss functions.\n",
    "While I was working on this topic I came across various research paper that explains\n",
    "solving this problem and to be honest my solution is not as good as in those papers. I\n",
    "would suggest you to go through those, just search depth estimation on\n",
    "\"paperwithcode.com\".\n",
    "\n",
    "![depth](https://paperswithcode.com/media/thumbnails/task/task-0000000605-d9849a91.jpg)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "##Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "tf.compat.v1.set_random_seed(123)\n",
    "session_conf = tf.compat.v1.ConfigProto(\n",
    "    intra_op_parallelism_threads=1, inter_op_parallelism_threads=1\n",
    ")\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Download the dataset\n",
    "\n",
    "We will be using the **DIODE: A Dense Indoor and Outdoor Depth Dataset**  for this\n",
    "tutorial. We have used the validation set for training and validating our model. The\n",
    "reason we have used validation set and not training set of the orginal dataset because\n",
    "the training set consist of 81GB data which was a bit difficult to download compared to\n",
    "validation set which is only 2.6GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "annotation_folder = \"/dataset/\"\n",
    "if not os.path.exists(os.path.abspath(\".\") + annotation_folder):\n",
    "    annotation_zip = tf.keras.utils.get_file(\n",
    "        \"val.tar.gz\",\n",
    "        cache_subdir=os.path.abspath(\".\"),\n",
    "        origin=\"http://diode-dataset.s3.amazonaws.com/val.tar.gz\",\n",
    "        extract=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "##  Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "path = \"val\"\n",
    "# we shall store all the file names in this list\n",
    "filelist = []\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        # append the file name to the list\n",
    "        filelist.append(os.path.join(root, file))\n",
    "\n",
    "filelist.sort()\n",
    "data = {\n",
    "    \"image\": [x for x in filelist if x.endswith(\".png\")],\n",
    "    \"depth\": [x for x in filelist if x.endswith(\"_depth.npy\")],\n",
    "    \"mask\": [x for x in filelist if x.endswith(\"_depth_mask.npy\")],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "\n",
    "class config:\n",
    "    HEIGHT = 256\n",
    "    WIDTH = 256\n",
    "    LR = 1e-4\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 32\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Building Dataset Loader Pipeline\n",
    "\n",
    "1. The pipeline takes dataframe mainintaing the path for RGB, depth and depth mask files\n",
    "as input.\n",
    "2. Reads and resize the RGB images\n",
    "3. Reads the depth and depth_mask files, process them to generate the depth-map image and\n",
    "resize it.\n",
    "4. Returns the RGB images and the depth-map images for a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, batch_size=6, dim=(768, 1024), n_channels=3, shuffle=True):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.indices = self.data.index.tolist()\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if (index + 1) * self.batch_size > len(self.indices):\n",
    "            self.batch_size = len(self.indices) - index * self.batch_size\n",
    "        # Generate one batch of data\n",
    "        # Generate indices of the batch\n",
    "        index = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        # Find list of IDs\n",
    "        batch = [self.indices[k] for k in index]\n",
    "        X, y = self.__data_generation(batch)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __load__(self, image_path, depth_map, mask):\n",
    "        \"Load Target image\"\n",
    "\n",
    "        image_ = cv2.imread(image_path)\n",
    "        image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "        image_ = cv2.resize(image_, self.dim)\n",
    "        image_ = (image_) / 255.0\n",
    "\n",
    "        depth_map = np.load(depth_map).squeeze()\n",
    "\n",
    "        mask = np.load(mask)\n",
    "        mask = mask > 0\n",
    "\n",
    "        MIN_DEPTH = 0.1\n",
    "\n",
    "        MAX_DEPTH = min(300, np.percentile(depth_map, 99))\n",
    "        depth_map = np.clip(depth_map, MIN_DEPTH, MAX_DEPTH)\n",
    "        depth_map = np.log(depth_map, where=mask)\n",
    "\n",
    "        depth_map = np.ma.masked_where(~mask, depth_map)\n",
    "\n",
    "        depth_map = np.clip(depth_map, 0.1, np.log(MAX_DEPTH))\n",
    "        depth_map = cv2.resize(depth_map, self.dim)\n",
    "        depth_map = np.expand_dims(depth_map, axis=2)\n",
    "\n",
    "        # depth_map = tf.image.convert_image_dtype(depth_map, tf.float32)\n",
    "\n",
    "        return image_, depth_map\n",
    "\n",
    "    def __data_generation(self, batch):\n",
    "\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, *self.dim, 1))\n",
    "\n",
    "        for i, id_ in enumerate(batch):\n",
    "            X[i,], y[i,] = self.__load__(\n",
    "                self.data[\"image\"][id_],\n",
    "                self.data[\"depth\"][id_],\n",
    "                self.data[\"mask\"][id_],\n",
    "            )\n",
    "\n",
    "        return X, y\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualizing Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "visualize_samples = next(\n",
    "    iter(DataGenerator(data=df, batch_size=6, dim=(config.HEIGHT, config.WIDTH)))\n",
    ")\n",
    "input, target = visualize_samples\n",
    "cmap = plt.cm.jet\n",
    "cmap.set_bad(color=\"black\")\n",
    "fig, ax = plt.subplots(6, 2, figsize=(50, 50))\n",
    "for i in range(6):\n",
    "    ax[i, 0].imshow((input[i].squeeze()))\n",
    "    ax[i, 1].imshow((target[i].squeeze()), cmap=cmap)\n",
    "\n",
    "d = np.flipud(target[2].squeeze())\n",
    "img = np.flipud(input[2].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 3D Point-Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "STEP = 2\n",
    "for x in range(0, img.shape[0], STEP):\n",
    "    for y in range(0, img.shape[1], STEP):\n",
    "        ax.scatter([d[x, y]] * 3, [y] * 3, [x] * 3, c=tuple(img[x, y, :3] / 255), s=3)\n",
    "    ax.view_init(15, 165)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### angle-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "ax.view_init(30, 135)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### angle-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "ax.view_init(5, 100)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### angle-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "ax.view_init(45, 220)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Building the model\n",
    "1. The basic model architecture has been taken from U-NET.\n",
    "2. Residual-blocks has been used in the down-scale blocks of the U-NET architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DownscaleBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "        super().__init__()\n",
    "        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.reluA = layers.LeakyReLU(alpha=0.2)\n",
    "        self.reluB = layers.LeakyReLU(alpha=0.2)\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.pool = layers.MaxPool2D((2, 2), (2, 2))\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        d = self.convA(input_tensor)\n",
    "        x = self.bn2a(d)\n",
    "        x = self.reluA(x)\n",
    "\n",
    "        x = self.convB(x)\n",
    "        x = self.bn2b(x)\n",
    "        x = self.reluB(x)\n",
    "\n",
    "        x += d\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "\n",
    "class UpscaleBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "        super().__init__()\n",
    "        self.us = layers.UpSampling2D((2, 2))\n",
    "        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.reluA = layers.LeakyReLU(alpha=0.2)\n",
    "        self.reluB = layers.LeakyReLU(alpha=0.2)\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "        self.conc = layers.Concatenate()\n",
    "\n",
    "    def call(self, x, skip):\n",
    "        x = self.us(x)\n",
    "        concat = self.conc([x, skip])\n",
    "        x = self.convA(concat)\n",
    "        x = self.bn2a(x)\n",
    "        x = self.reluA(x)\n",
    "\n",
    "        x = self.convB(x)\n",
    "        x = self.bn2b(x)\n",
    "        x = self.reluB(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleNeckBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "        super().__init__()\n",
    "        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.reluA = layers.LeakyReLU(alpha=0.2)\n",
    "        self.reluB = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.convA(x)\n",
    "        x = self.reluA(x)\n",
    "        x = self.convB(x)\n",
    "        x = self.reluB(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def UnetModel(height, width):\n",
    "\n",
    "    f = [16, 32, 64, 128, 256]\n",
    "\n",
    "    inputs = layers.Input((height, width, 3))\n",
    "\n",
    "    c1, p1 = DownscaleBlock(f[0])(inputs)\n",
    "    c2, p2 = DownscaleBlock(f[1])(p1)\n",
    "    c3, p3 = DownscaleBlock(f[2])(p2)\n",
    "    c4, p4 = DownscaleBlock(f[3])(p3)\n",
    "\n",
    "    bn = BottleNeckBlock(f[4])(p4)\n",
    "\n",
    "    u1 = UpscaleBlock(f[3])(bn, c4)\n",
    "    u2 = UpscaleBlock(f[2])(u1, c3)\n",
    "    u3 = UpscaleBlock(f[1])(u2, c2)\n",
    "    u4 = UpscaleBlock(f[0])(u3, c1)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"tanh\")(u4)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Optimizing Loss\n",
    "We have tried to optimize 3 losses in our model.\n",
    "1. Structural similarity index(SSIM).\n",
    "2. L1-loss, or Point-wise depth in our case.\n",
    "3. Edge wide depth with depth smoothness.\n",
    "\n",
    "Out of the three loss functions SSIM contributed the most in improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DepthEstimationModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        super(DepthEstimationModel, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        input, target = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.model(input, training=True)\n",
    "\n",
    "            # Edges\n",
    "            dy_true, dx_true = tf.image.image_gradients(target)\n",
    "            dy_pred, dx_pred = tf.image.image_gradients(pred)\n",
    "            weights_x = tf.exp(tf.reduce_mean(tf.abs(dx_true)))\n",
    "            weights_y = tf.exp(tf.reduce_mean(tf.abs(dy_true)))\n",
    "\n",
    "            # depth_smoothness\n",
    "            smoothness_x = dx_pred * weights_x\n",
    "            smoothness_y = dy_pred * weights_y\n",
    "\n",
    "            l_edges = tf.reduce_mean(abs(smoothness_x)) + tf.reduce_mean(\n",
    "                abs(smoothness_y)\n",
    "            )\n",
    "\n",
    "            # Structural similarity (SSIM) index\n",
    "            l_ssim = tf.reduce_mean(\n",
    "                1\n",
    "                - tf.image.ssim(\n",
    "                    target, pred, max_val=256, filter_size=7, k1=0.01 ** 2, k2=0.03 ** 2\n",
    "                )\n",
    "            )\n",
    "            # Point-wise depth\n",
    "            l1_loss = tf.reduce_mean(tf.abs(target - pred))\n",
    "\n",
    "            # Weights\n",
    "            w1 = 0.85\n",
    "            w2 = 0.1\n",
    "            w3 = 1.0\n",
    "\n",
    "            loss = (w1 * l_ssim) + w2 * l1_loss + (w3 * l_edges)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\n",
    "            \"Loss\": self.loss_metric.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, x):\n",
    "        pass\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=config.LR,\n",
    "    amsgrad=True,\n",
    ")\n",
    "model = UnetModel(config.HEIGHT, config.WIDTH)\n",
    "DEM = DepthEstimationModel(model=model)\n",
    "DEM.compile(optimizer)\n",
    "\n",
    "train_loader = DataGenerator(\n",
    "    data=df[:600].reset_index(drop=\"true\"),\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    dim=(config.HEIGHT, config.WIDTH),\n",
    ")\n",
    "DEM.fit(train_loader, epochs=config.EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualizing Model output.\n",
    "Visualizing model output over validation set.\n",
    "The first image is the RGB image, the second image is the ground truth depth-map image\n",
    "and the third one is the predicted depth-map image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "test_loader = next(\n",
    "    iter(\n",
    "        DataGenerator(\n",
    "            data=df[601:].reset_index(drop=\"true\"),\n",
    "            batch_size=6,\n",
    "            dim=(config.HEIGHT, config.WIDTH),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "input, target = test_loader\n",
    "pred = model.predict(input)\n",
    "cmap = plt.cm.jet\n",
    "cmap.set_bad(color=\"black\")\n",
    "fig, ax = plt.subplots(6, 3, figsize=(50, 50))\n",
    "for i in range(6):\n",
    "    ax[i, 0].imshow((input[i].squeeze()))\n",
    "    ax[i, 1].imshow((target[i].squeeze()), cmap=cmap)\n",
    "    ax[i, 2].imshow((pred[i].squeeze()), cmap=cmap)\n",
    "\n",
    "test_loader = next(\n",
    "    iter(\n",
    "        DataGenerator(\n",
    "            data=df[701:].reset_index(drop=\"true\"),\n",
    "            batch_size=6,\n",
    "            dim=(config.HEIGHT, config.WIDTH),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "input, target = test_loader\n",
    "pred = model.predict(input)\n",
    "cmap = plt.cm.jet\n",
    "cmap.set_bad(color=\"black\")\n",
    "fig, ax = plt.subplots(6, 3, figsize=(50, 50))\n",
    "for i in range(6):\n",
    "    ax[i, 0].imshow((input[i].squeeze()))\n",
    "    ax[i, 1].imshow((target[i].squeeze()), cmap=cmap)\n",
    "    ax[i, 2].imshow((pred[i].squeeze()), cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Scopes of Improvement\n",
    "\n",
    "1. From the research papers that I read while I was working on this topic the encode part\n",
    "of the unet was replaced with DenseNet, ResNet or other pre-trained model, which could be\n",
    "applied for better model predictions.\n",
    "\n",
    "2. Loss functions plays an immense role in solving this problem, and different paper\n",
    "explained different ways of developing the loss function out of which SSIM was common in\n",
    "all. so playing with the loss fuctions gives a huge scope of improvement in this case."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "depth_estimation",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}