{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# VICReg: Variance-Invariance-Covariance Regularization for SSL\n",
    "\n",
    "**Author:** Abhiraam Eranti<br>\n",
    "**Date created:** 4/13/2022<br>\n",
    "**Last modified:** 4/13/2022<br>\n",
    "**Description:** We implement VICReg using Tensorflow Similarity and train on CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Problem**\n",
    "VicReg, created by Adrien Bardes, Jean Ponce, and Yann LeCun, is a self-supervised method\n",
    "to generate high-quality embeddings that\n",
    "maximize the amount of dataset-related information inside them.\n",
    "Previously, the main way to get these kinds of embeddings was to just calculate\n",
    "the distance between representations of similar and different images.\n",
    "Ideally, similar images would have similar embeddings and different images\n",
    "have different ones. However, there was one problem: they\n",
    "would *collapse*, or try to \"cheat the system\". Let's look at an example:\n",
    "Suppose we had an image of a cat and a dog. The embeddings should\n",
    "primarily store information from the images that differentiate the cat from its canine\n",
    "counterpart. For example, it could keep the shape of the ears of both images, or maybe\n",
    "the tail length, and so on. When used in a downstream task,\n",
    "like a classification model, these embeddings(which have the curated measurements\n",
    "mentioned above) should assist the model.\n",
    "However, instead of this occurring, these approaches would produce embeddings that did\n",
    "not help the downstream model as much as they should have. This is because they would\n",
    "become redundant, meaning they repeated information more\n",
    "than once. This led to less information being passed to the downstream model.\n",
    "The previous solutions were to carefully and precisely tune the weights and\n",
    "augmentations of the model and data such that collapse does not occur. However,\n",
    "this was a finicky task, and even then, redundancy was still an issue.\n",
    "**Solutions**\n",
    "VicReg was not the first solution to this. Barlow Twins is\n",
    "another similar method that was designed to reduce redundancy by measuring both\n",
    "the invariance and covariance of embeddings. It works pretty well at\n",
    "doing this, and is generally better in performance to contrastive models like\n",
    "SimCLR.\n",
    "VicReg is inspired by Barlow Twins and shares a similar performance to it\n",
    "on tasks like image classification, the example shown here. Instead of just\n",
    "measuring the invariance and covariance, it measures *similarity*, *variance*,\n",
    "and *covariance* concerning the embeddings instead. However, they share the\n",
    "same model composition and training loop, and both are substantially simpler\n",
    "than other methods to train.\n",
    "However, VicReg outperforms Barlow Twins on multimodal tasks like\n",
    "image-to-text and text-to-image translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will also utilize **Tensorflow Similarity**, a library designed to make metric and\n",
    "self-supervised learning easier for practical use. Using this\n",
    "library, we do not need to make the augmentations, model architectures,\n",
    "training loop, and visualization code ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### References\n",
    "[VicReg Paper](https://arxiv.org/abs/2105.04906)\n",
    "[VicReg PyTorch Implementation](https://github.com/facebookresearch/vicreg)\n",
    "[Barlow Twins Paper](https://arxiv.org/abs/2103.03230)\n",
    "[Barlow Twins Example(Some of the architecture code is copied from\n",
    "it)](https://keras.io/examples/vision/barlow_twins/)\n",
    "[Tensorflow Similarity](https://github.com/tensorflow/similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We need `tensorflow-addons` for the LAMB loss function and\n",
    "`tensorflow-similarity` for our augmenting, model building, and training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!pip install tensorflow-addons\n",
    "!!pip install tensorflow-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# slightly faster improvements, on the first epoch 30 second decrease and a 1-2 second\n",
    "# decrease in epoch time. Overall saves approx. 5 min of training time\n",
    "\n",
    "# Allocates two threads for a gpu private which allows more operations to be\n",
    "# done faster\n",
    "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\"\n",
    "\n",
    "import tensorflow as tf  # framework\n",
    "from tensorflow import keras  # for tf.keras\n",
    "import tensorflow_addons as tfa  # LAMB optimizer and gaussian_blur_2d function\n",
    "import numpy as np  # np.random.random\n",
    "import matplotlib.pyplot as plt  # graphs\n",
    "import datetime  # tensorboard logs naming\n",
    "import tensorflow_similarity  # loss function module\n",
    "from functools import partial\n",
    "\n",
    "# XLA optimization for faster performance(up to 10-15 minutes total time saved)\n",
    "tf.config.optimizer.set_jit(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will be using CIFAR-10 as it is a nice baseline for our task. Because it has been used\n",
    "for several other models, we can compare our results with other methods.\n",
    "For the sake of time, we will only use 30% of the dataset, or around 18000 images for\n",
    "this experiment. 15000 will be unlabeled images used during the VicReg process, and only\n",
    "3000 labeled images will be used to train our linear evaluation model. Because of this,\n",
    "we will see subpar results from our model. Try running this project in an interactive\n",
    "notebook while changing that `DATASET_PERCENTAGE` constant to be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Batch size of dataset\n",
    "BATCH_SIZE = 512\n",
    "# Width and height of image\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "[\n",
    "    (train_features, train_labels),\n",
    "    (test_features, test_labels),\n",
    "] = keras.datasets.cifar10.load_data()\n",
    "\n",
    "DATASET_PERCENTAGE = 0.3\n",
    "train_features = train_features[: int(len(train_features) * DATASET_PERCENTAGE)]\n",
    "test_features = test_features[: int(len(test_features) * DATASET_PERCENTAGE)]\n",
    "train_labels = train_labels[: int(len(train_labels) * DATASET_PERCENTAGE)]\n",
    "test_labels = test_labels[: int(len(test_labels) * DATASET_PERCENTAGE)]\n",
    "\n",
    "train_features = train_features / 255.0\n",
    "test_features = test_features / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "VicReg uses the same augmentation pipeline as both Barlow Twins and BYOL. These\n",
    "augmentations occur probabilistically, which allows for even more variation to help the\n",
    "model learn.\n",
    "<details>\n",
    "<summary>Augmentation Pipeline Details</summary>\n",
    "The pipeline is as follows:\n",
    "* ***Random Crop***: We crop a random part of the image out. This resulting cropped image\n",
    "is between 75% and 100% of the image size. Then, the cropped image is resized to the\n",
    "original image width and height.\n",
    "* ***Random Horizontal Flip*** (*50%*): There is a *50%* probability that the image will\n",
    "be flipped horizontally\n",
    "* ***Random Color Jitter*** (*80%*): There is an *80%* probability that the image will be\n",
    "discolored. This process includes:\n",
    "  * Random brightness (additive), ranging from `-0.8` to `+0.8`.\n",
    "  * Random contrast (multiplicative), ranging from `0.4` to `1.6`\n",
    "  * Random saturation (multiplicative), ranging from `0.4` to `1.6`\n",
    "  * Random hue (multiplicative), ranging from `0.8` to `1.2`\n",
    "* ***Random Greyscale*** (*20%*)\n",
    "* ***Random Gaussian Blur***(*20%*): The blur amount \u03c3 ranges from `0` to\n",
    "`1`\n",
    "* ***Random Solarization***(*20%*): Solarization is when very low pixels get\n",
    "inverted to do to irregularities in the camera. The solarization threshold for this\n",
    "pipeline is `10`. If a pixel(not normalized) is below `10`, it will be\n",
    "flipped to `255-pixel`.\n",
    "Instead of implementing these pipelines ourselves, Tensorflow Similarity has\n",
    "a collection of augmenters that we can use instead. In this case, we will be\n",
    "using the pipeline function\n",
    "`tensorflow_similarity.augmenters.barlow.augment_barlow` that takes in an image\n",
    "and returns an augmented version using these transforms.\n",
    "</details>\n",
    "<details>\n",
    "<summary> Dataset method </summary>\n",
    "We'll use this function in the `tf.data.Dataset` API due to its ease of\n",
    "use when batching and mapping. However, Tensorflow Similarity offers a simpler method\n",
    "with it's augmenter library.\n",
    "You can use `tensorflow_similarity.augmenters.BarlowAugmenter()` as a callable.\n",
    "However, be aware that it *does* load the dataset into RAM, and you may have to\n",
    "handle extra preprocessing (like batching) separately.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Saves a few minutes of performance - disables intra-op parallelism\n",
    "performance_options = tf.data.Options()\n",
    "performance_options.threading.max_intra_op_parallelism = 1\n",
    "\n",
    "# Adding image width and height to augmenter\n",
    "configed_augmenter = partial(\n",
    "    tensorflow_similarity.augmenters.barlow.augment_barlow,\n",
    "    height=IMAGE_SIZE,\n",
    "    width=IMAGE_SIZE,\n",
    ")\n",
    "\n",
    "\n",
    "def make_version():\n",
    "    augment_version = (\n",
    "        tf.data.Dataset.from_tensor_slices(train_features)\n",
    "        .map(configed_augmenter, tf.data.AUTOTUNE)\n",
    "        .shuffle(1000, seed=1024)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "        .with_options(performance_options)\n",
    "    )\n",
    "\n",
    "    return augment_version\n",
    "\n",
    "\n",
    "augment_version_a, augment_version_b = make_version(), make_version()\n",
    "augment_versions = tf.data.Dataset.zip(\n",
    "    (augment_version_a, augment_version_b)\n",
    ").with_options(performance_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We can use `tensorflow_similarity.visualization.visualize_views` to check out a\n",
    "few sample images. Let's verify that each pair of images have a different set of\n",
    "transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "sample = next(iter(augment_versions))\n",
    "\n",
    "print(\"Augmented Views\")\n",
    "tensorflow_similarity.visualization.visualize_views(\n",
    "    sample, num_imgs=20, views_per_col=4, max_pixel_value=1.0, fig_size=(10, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "VicReg - Like Barlow Twins, requires a backbone (encoder) and a projector. The\n",
    "projector is responsible for creating the embeddings that represent the dataset\n",
    "as a whole.\n",
    "We will be using a ResNet-18 with an output of length 512 and attach that to a\n",
    "projector which will return embeddings of length 5000. The projector takes the\n",
    "output of the backbone, and applies a series of Dense, Batch Normalization, and\n",
    "Relu transformations.\n",
    "![Model Structure](https://i.imgur.com/GuVyyJW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "VicReg differs from other self-supervised methods like Barlow Twins due to its\n",
    "unique way of calculating the loss function. It checks the variance, covariance,\n",
    "and similarity between the embeddings per each image. We will be using\n",
    "`tensorflow_similarity.losses.Vicreg()` to do this for us.\n",
    "![Vicreg Loss](https://i.imgur.com/9Xa6tYD.png)\n",
    "When the image says to \"minimize the similarity\", it means to minimize the\n",
    "mean squared error.\n",
    "<details>\n",
    "<summary> Details about VicReg Loss </summary>\n",
    "The VicReg loss aims to:\n",
    "* ***Maximize*** the **variance** between corresponding elements of *different*\n",
    "embeddings within a\n",
    "batch. The notion is that different images should have different representations\n",
    "from other ones. One way to measure this is by taking the variance, which checks\n",
    "how varied, or scattered a dataset is. In this case, if the variance is high, we\n",
    "can assume that the embeddings for different images are going to be different.\n",
    "* ***Minimize*** the internal **covariance** of each embedding in the batch.\n",
    "Covariance is when the individual values in the embeddings \"correlate\" with each\n",
    "other. For example, if in an embedding there are two different variables that\n",
    "always have the same value with each other, we say they are covariant. This is\n",
    "bad because we want our embeddings to carry as much information about the\n",
    "dataset as possible, so that downstream tasks have a lot more to work with. If\n",
    "two different values in the embedding share correlations with each other, we\n",
    "wouldn't need two separate values; we can just have one embedding that carries\n",
    "both of their information together. Having two embeddings that always carry the\n",
    "same information is *redundant*, and we want to remove this redundancy to get\n",
    "the maximum information we can from these embeddings.\n",
    "* ***Minimize*** the **distance** between embeddings that are for the same image. Two\n",
    "similar images must have similar embeddings. To check this we can just use the\n",
    "Mean Squared Error to find the distance between them.\n",
    "Each of these losses are weighted summed with each other to get one loss number\n",
    "</details>\n",
    "<details>\n",
    "<summary> VicReg pseudocode of variance, covariance, similarity </summary>\n",
    "Variance Pseudocode:\n",
    "```\n",
    "z = mean_center(z)\n",
    "std_z = sqrt(var(a, axis=0) + SMALL_CONSTANT)\n",
    "std_z = mean(max(std_z, 0))\n",
    "*same for z' and std_z'*\n",
    "std_loss = average(std_z, std_z')\n",
    "```\n",
    "Similarity Pseudocode:\n",
    "```\n",
    "sim_loss = mse(z, z')\n",
    "```\n",
    "Covariance Pseudocode:\n",
    "```\n",
    "z = mean_center(z)\n",
    "cov_loss_z = mm(transpose(z), z).get_off_diagonal()\n",
    "cov_loss_z = sum(cov_loss_z) / embedding_size\n",
    "*do same for z' and cov_loss_z'*\n",
    "cov_loss = cov_loss_z + cov_loss_z'\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will be using Tensorflow Similarity's `ResNet18Sim` as our backbone and will implement\n",
    "our custom projector. The backbone and projector will be combined via `ContrastiveModel`,\n",
    "an API that manages our model composition and training\n",
    "loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Code for defining our projector\n",
    "def projector_layers(input_size, n_dense_neurons) -> keras.Model:\n",
    "    \"\"\"projector_layers method.\n",
    "    Builds the projector network for VicReg, which are a series of Dense,\n",
    "    BatchNormalization, and ReLU layers stacked on top of each other.\n",
    "    Returns:\n",
    "        returns the projector layers for VicReg\n",
    "    \"\"\"\n",
    "\n",
    "    # number of dense neurons in the projector\n",
    "    input_layer = tf.keras.layers.Input(input_size)\n",
    "\n",
    "    # intermediate layers of the projector network\n",
    "    n_layers = 2\n",
    "    for i in range(n_layers):\n",
    "        dense = tf.keras.layers.Dense(n_dense_neurons, name=f\"projector_dense_{i}\")\n",
    "        x = dense(input_layer) if i == 0 else dense(x)\n",
    "        x = tf.keras.layers.BatchNormalization(name=f\"projector_bn_{i}\")(x)\n",
    "        x = tf.keras.layers.ReLU(name=f\"projector_relu_{i}\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(n_dense_neurons, name=f\"projector_dense_{n_layers}\")(x)\n",
    "\n",
    "    model = keras.Model(input_layer, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "backbone = tensorflow_similarity.architectures.ResNet18Sim(\n",
    "    (IMAGE_SIZE, IMAGE_SIZE, 3), embedding_size=512\n",
    ")\n",
    "projector = projector_layers(backbone.output.shape[-1], n_dense_neurons=5000)\n",
    "\n",
    "model = tensorflow_similarity.models.ContrastiveModel(\n",
    "    backbone=backbone,\n",
    "    projector=projector,\n",
    "    algorithm=\"barlow\",  # VicReg uses same architecture + training loop as Barlow Twins\n",
    ")\n",
    "\n",
    "# LAMB optimizer converges faster than ADAM or SGD when using large batch sizes.\n",
    "optimizer = tfa.optimizers.LAMB()\n",
    "loss = tensorflow_similarity.losses.VicReg()\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Expected training time: 1 hour\n",
    "history = model.fit(augment_versions, epochs=75)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Evaluation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will use *linear evaluation* to see how well our model learned embeddings.\n",
    "This is where we freeze our trained backbone and projector, and just add a\n",
    "single Dense + Softmax layer. Then, we train our model using the test images and\n",
    "labels. Because we took 30% of CIFAR-10 when sampling, we are only training this\n",
    "model with 3000 labeled images. However, remember that we trained the backbone\n",
    "and projector using 12000 images, though they were unlabeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def gen_lin_ds(features, labels):\n",
    "    ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "        .shuffle(1000)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "xy_ds = gen_lin_ds(train_features, train_labels)\n",
    "test_ds = gen_lin_ds(test_features, test_labels)\n",
    "\n",
    "evaluator = keras.models.Sequential(\n",
    "    [\n",
    "        model.backbone,\n",
    "        model.projector,\n",
    "        keras.layers.Dense(\n",
    "            10, activation=\"softmax\", kernel_regularizer=keras.regularizers.l2(0.02)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Need to test the backbone\n",
    "evaluator.layers[0].trainable = False\n",
    "evaluator.layers[1].trainable = False\n",
    "\n",
    "linear_optimizer = tfa.optimizers.LAMB()\n",
    "evaluator.compile(\n",
    "    optimizer=linear_optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "evaluator.fit(xy_ds, epochs=35, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Our accuracy should be between 60%-63%. This shows that our VicReg model was\n",
    "able to learn a lot from the dataset, and can get better results than just the\n",
    "10% one may get with random guessing.\n",
    "**Things To try**\n",
    "* If you change `DATASET_PERCENTAGE` to 1, meaning that it would use all the\n",
    "dataset, accuracy should increase to about 70%\n",
    "* If the number of epochs is changed from 75 to 150, accuracy may also increase\n",
    "by a few points as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "* VicReg is a method of self-supervised partially-contrastive learning to\n",
    "generate high-quality embeddings that contain dataset relationships.\n",
    "* Using VicReg on 30% of our dataset, out of which 80% is unlabeled, we can get\n",
    "an accuracy of around 62% when freezing all layers except a small Dense layer\n",
    "at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "* VicReg, and other similar algorithms, have several use cases.\n",
    "  * Can be used in semi-supervised learning, as shown in this demo. This is\n",
    "  where you have a lot of unlabeled data and very little labeled data. You can\n",
    "  use the unlabeled data to generate embeddings to assist the labeled data when\n",
    "  training.\n",
    "* VicReg vs Barlow Twins (Predecessor)\n",
    "  * VicReg performs similarly to Barlow Twins on CIFAR-10 and other\n",
    "  Image classification datasets\n",
    "  * However it significantly outperforms Barlow Twins on multi-modal tasks like\n",
    "  Image-to-Text and Text-to-Image\n",
    "  ![Table](https://i.imgur.com/GuWIssF.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vicreg",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}