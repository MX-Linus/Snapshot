{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Image classification using EfficientNet and fine-tuning\n",
    "\n",
    "**Author:** Yixing Fu<br>\n",
    "**Date created:** 2020/06/30<br>\n",
    "**Last modified:** 2020/06/30<br>\n",
    "**Description:** Use EfficientNet with weights pre-trained on imagenet for CIFAR-100 classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# image classification using EfficientNet and fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## What is EfficientNet\n",
    "EfficientNet, first introduced in https://arxiv.org/abs/1905.11946 is among the most\n",
    "efficient models (i.e. requiring least FLOPS for inference) that reaches SOTA in both\n",
    "imagenet and common image classification transfer learning tasks.\n",
    "\n",
    "The smallest base model is similar to MnasNet (https://arxiv.org/abs/1807.11626), which\n",
    "reached near-SOTA with a significantly smaller model. By introducing a heuristic way to\n",
    "scale the model, EfficientNet provides a family of models (B0 to B7) that represents a\n",
    "good combination of efficiency and accuracy on a variety of scales. Such a scaling\n",
    "heuristics (compound-scaling, details see https://arxiv.org/abs/1905.11946) allows the\n",
    "efficiency-oriented base model (B0) to surpass models at every scale, while avoiding\n",
    "extensive grid-search of hyperparameters.\n",
    "\n",
    "A summary of the latest updates on the model is available at\n",
    "https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet, where various\n",
    "augmentation schemes and semi-supervised learning approaches are applied to further\n",
    "improve the imagenet performance of the models. These extensions of the model can be used\n",
    "by updating weights without changing model architecture.\n",
    "\n",
    "## Compound scaling\n",
    "\n",
    "The EfficientNet models are approximately created using compound scaling. Starting from\n",
    "the base model B0, as model size scales from B0 to B7, the extra computational resource\n",
    "is proportioned into width, depth and resolution of the model by requiring each of the\n",
    "three dimensions to grow at the same power of a set of fixed ratios.\n",
    "\n",
    "However, it must be noted that the ratios are not taken accurately. A few points need to\n",
    "be taken into account:\n",
    "Resolution. Resolutions not divisible by 8, 16, etc. cause zero-padding near boundaries\n",
    "of some layers which wastes computational resources. This especially applies to smaller\n",
    "variants of the model, hence the input resolution for B0 and B1 are chosen as 224 and\n",
    "240.\n",
    "Depth and width. Channel size is always rounded to 8/16/32 because of the architecture.\n",
    "Resource limit. Perfect compound scaling would assume spatial (memory) and time allowance\n",
    "for the computation to grow simultaneously, but OOM may further bottleneck the scaling of\n",
    "resolution.\n",
    "\n",
    "As a result, compound scaling factor is significantly off from\n",
    "https://arxiv.org/abs/1905.11946. Hence it is important to understand the compound\n",
    "scaling as a rule of thumb that leads to this family of base models, rather than an exact\n",
    "optimization scheme. This also justifies that in the keras implementation (detailed\n",
    "below), only these 8 models, B0 to B7, are exposed to the user and arbitrary width /\n",
    "depth / resolution is not allowed.\n",
    "\n",
    "## Keras implementation of EfficientNet\n",
    "\n",
    "An implementation of EfficientNet B0 to B7 has been shipped with tf.keras since TF2.3. To\n",
    "use EfficientNetB0 for classifying 1000 classes of images from imagenet, run\n",
    "```\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "model = EfficientNetB0(weights='imagenet')\n",
    "```\n",
    "\n",
    "This model takes input images of shape (224, 224, 3), and the input data should range\n",
    "[0,255]. Resizing and normalization are included as part of the model.\n",
    "\n",
    "Because training EfficientNet on imagenet takes a tremendous amount of resources and\n",
    "several techniques that are not a part of the model architecture itself. Hence the Keras\n",
    "implementation by default loads pre-trained weights with AutoAugment\n",
    "(https://arxiv.org/abs/1805.09501).\n",
    "\n",
    "For B0 to B7 base models, the input shapes are different. Here is a list of input shape\n",
    "expected for each model:\n",
    "\n",
    "| Base model | resolution|\n",
    "|----------------|-----|\n",
    "| EfficientNetB0 | 224 |\n",
    "| EfficientNetB1 | 240 |\n",
    "| EfficientNetB2 | 260 |\n",
    "| EfficientNetB3 | 300 |\n",
    "| EfficientNetB4 | 380 |\n",
    "| EfficientNetB5 | 456 |\n",
    "| EfficientNetB6 | 528 |\n",
    "| EfficientNetB7 | 600 |\n",
    "\n",
    "When the use of the model is intended for transfer learning, the Keras implementation\n",
    "provides a option to remove the top layers:\n",
    "```\n",
    "model = EfficientNetB0(include_top=False, weights='imagenet')\n",
    "```\n",
    "This option excludes the final Dense layer that turns 1280 features on the penultimate\n",
    "layer into prediction of the 1000 classes in imagenet. Replacing the top with custom\n",
    "layers allows using EfficientNet as a feature extractor and transfers the pretrained\n",
    "weights to other tasks.\n",
    "\n",
    "Another keyword in the model builder worth noticing is `drop_connect_rate` which controls\n",
    "the dropout rate responsible for stochastic depth (https://arxiv.org/abs/1603.09382).\n",
    "This parameter serves as a toggle for extra regularization in finetuning, but does not\n",
    "alter loaded weights.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Example: EfficientNetB0 for CIFAR-100.\n",
    "\n",
    "As an architecture, EfficientNet is capable of a wide range of image classification\n",
    "tasks. For example, we will show using pre-trained EfficientNetB0 on CIFAR-100. For\n",
    "EfficientNetB0, image size is 224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# IMG_SIZE is determined by EfficientNet model choice\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!pip install --quiet tensorflow==2.3.0rc0\n",
    "!!pip install --quiet cloud-tpu-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from cloud_tpu_client import Client\n",
    "\n",
    "try:\n",
    "    c = Client()\n",
    "    c.configure_tpu_version(tf.__version__, restart_type=\"always\")\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Below is example code for loading data.\n",
    "To see sensible result, you need to load entire dataset and adjust epochs for\n",
    "training; but you may truncate data for a quick verification of the workflow.\n",
    "Expect the notebook to run at least an hour for GPU, while much faster on TPU if\n",
    "using hosted Colab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "NUM_CLASSES = 100\n",
    "\n",
    "x_train = tf.cast(x_train, tf.int32)\n",
    "x_test = tf.cast(x_test, tf.int32)\n",
    "\n",
    "truncate_data = False  # @param {type: \"boolean\"}\n",
    "if truncate_data:\n",
    "    x_train = x_train[0:5000]\n",
    "    y_train = y_train[0:5000]\n",
    "    x_test = x_test[0:1000]\n",
    "    y_test = y_test[0:1000]\n",
    "\n",
    "\n",
    "# one-hot / categorical\n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.batch(batch_size=batch_size, drop_remainder=True)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "ds_test = ds_test.batch(batch_size=batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### training from scratch\n",
    "To build model that use EfficientNetB0 with 100 classes that is initiated from scratch:\n",
    "\n",
    "Note: to better see validation peeling off from training accuracy, try increasing epochs\n",
    "to ~20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers.experimental.preprocessing import (\n",
    "    Resizing,\n",
    "    RandomFlip,\n",
    "    RandomContrast,\n",
    "    # RandomHeight,\n",
    ")\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "with strategy.scope():\n",
    "    inputs = keras.layers.Input(shape=(32, 32, 3))\n",
    "    x = inputs\n",
    "\n",
    "    x = RandomFlip()(x)\n",
    "    x = RandomContrast(0.1)(x)\n",
    "    # x = RandomHeight(0.1)(x)\n",
    "    x = Resizing(IMG_SIZE, IMG_SIZE, interpolation=\"bilinear\")(x)\n",
    "\n",
    "    x = EfficientNetB0(include_top=True, weights=None, classes=100)(x)\n",
    "\n",
    "    model = keras.Model(inputs, x)\n",
    "\n",
    "    sgd = SGD(learning_rate=0.2, momentum=0.1, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.005, verbose=2\n",
    ")\n",
    "\n",
    "epochs = 30  # @param {type: \"slider\", min:5, max:50}\n",
    "hist = model.fit(\n",
    "    ds_train, epochs=epochs, validation_data=ds_test, callbacks=[reduce_lr], verbose=2\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Training the model is relatively fast (takes only 20 seconds per epoch on TPUv2 that is\n",
    "available on colab). This might make it sounds easy to simply train EfficientNet on any\n",
    "dataset wanted from scratch. However, training EfficientNet on smaller datasets,\n",
    "especially those with lower resolution like CIFAR-100, faces the significant challenge of\n",
    "overfitting or getting trapped in local extrema.\n",
    "\n",
    "Hence traning from scratch requires very careful choice of hyperparameters and is\n",
    "difficult to find suitable regularization. Plotting the training and validation accuracy\n",
    "makes it clear that validation accuracy stagnates at very low value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### transfer learning from pretrained weight\n",
    "Using pre-trained imagenet weights and only transfer learn (fine-tune) the model allows\n",
    "utilizing the power of EfficientNet much easier. To use pretrained weight, the model can\n",
    "be initiated through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import (\n",
    "    Resizing,\n",
    "    RandomContrast,\n",
    ")\n",
    "\n",
    "\n",
    "def build_model(n_classes):\n",
    "    inputs = keras.layers.Input(shape=(32, 32, 3))\n",
    "    x = inputs\n",
    "\n",
    "    x = RandomFlip()(x)\n",
    "    x = RandomContrast(0.1)(x)\n",
    "    x = Resizing(IMG_SIZE, IMG_SIZE, interpolation=\"bilinear\")(x)\n",
    "    # other preprocessing layers can be used similar to Resizing and RandomRotation\n",
    "\n",
    "    model = EfficientNetB0(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "\n",
    "    # freeze the pretrained weights\n",
    "    for l in model.layers:\n",
    "        l.trainable = False\n",
    "\n",
    "    # rebuild top\n",
    "    x = keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = keras.layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    x = keras.layers.Dense(100, activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "    # compile\n",
    "    model = keras.Model(inputs, x, name=\"EfficientNet\")\n",
    "    sgd = SGD(learning_rate=0.2, momentum=0.1, nesterov=True)\n",
    "    # sgd = tfa.optimizers.MovingAverage(sgd)\n",
    "    model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Note that it is also possible to freeze pre-trained part entirely by\n",
    "```\n",
    "model.trainable = False\n",
    "```\n",
    "instead of setting each layer separately.\n",
    "\n",
    "\n",
    "The first step to transfer learning is to freeze all layers and train only the top\n",
    "layers. For this step a relatively large learning rate (~0.1) can be used to start with,\n",
    "while applying some learning rate decay (either ExponentialDecay or use ReduceLROnPlateau\n",
    "callback). On CIFAR-100 with EfficientNetB0, this step will take validation accuracy to\n",
    "~70% with suitable (but not absolutely optimal) image augmentation. For this stage, using\n",
    "EfficientNetB0, validation accuracy and loss will be consistently better than training\n",
    "accuracy and loss. This is because the regularization is relatively strong, and it only\n",
    "suppresses train time metrics.\n",
    "\n",
    "Note that the convergence may take up to 50 epochs. If no data augmentation layer is\n",
    "applied, expect the validation accuracy to reach only ~60% even for many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_model(n_classes=NUM_CLASSES)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.0001, verbose=2\n",
    ")\n",
    "\n",
    "epochs = 30  # @param {type: \"slider\", min:8, max:80}\n",
    "hist = model.fit(\n",
    "    ds_train, epochs=epochs, validation_data=ds_test, callbacks=[reduce_lr], verbose=2,\n",
    ")\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The second step is to unfreeze a number of layers. Unfreezing layers and fine tuning is\n",
    "usually thought to only provide incremental improvements on validation accuracy, but for\n",
    "the case of EfficientNetB0 it boosts validation accuracy by about 10% to pass 80%\n",
    "(reaching ~87% as in the original paper requires including AutoAugmentation or Random\n",
    "Augmentaion).\n",
    "\n",
    "Note that the convergence may take more than 50 epochs. If no data augmentation layer is\n",
    "applied, expect the validation accuracy to reach only ~70% even for many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def unfreeze_model(model):\n",
    "    for l in model.layers:\n",
    "        if \"bn\" in l.name:\n",
    "            print(f\"{l.name} is staying untrainable\")\n",
    "        else:\n",
    "            l.trainable = True\n",
    "\n",
    "    sgd = SGD(learning_rate=0.005)\n",
    "    model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = unfreeze_model(model)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.00001, verbose=2\n",
    ")\n",
    "epochs = 30  # @param {type: \"slider\", min:8, max:80}\n",
    "hist3 = model.fit(\n",
    "    ds_train, epochs=epochs, validation_data=ds_test, callbacks=[reduce_lr], verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### tips for fine tuning EfficientNet\n",
    "\n",
    "On unfreezing layers:\n",
    "- The batch normalization layers need to be kept untrainable\n",
    "(https://keras.io/guides/transfer_learning/). If they are also turned to trainable, the\n",
    "first epoch after unfreezing will significantly reduce accuracy.\n",
    "- In some cases it may be beneficial to open up only a portion of layers instead of\n",
    "unfreezing all. This will make fine tuning much faster when going to larger models like\n",
    "B7.\n",
    "- Each block needs to be all turned on or off. This is because the architecture includes\n",
    "a shortcut from the first layer to the last layer for each block. Not respecting blocks\n",
    "also significantly harms the final performance.\n",
    "\n",
    "\n",
    "Some other tips for utilizing EfficientNet\n",
    "- Larger variants of EfficientNet do not guarantee improved performance, especially for\n",
    "tasks with less data or fewer classes. In such a case, the larger variant of EfficientNet\n",
    "chosen, the harder it is to tune hyperparameters.\n",
    "- EMA (Exponential Moving Average) is very helpful in training EfficientNet from scratch,\n",
    "but not so much for transfer learning.\n",
    "- Do not use the RMSprop setup as in the original paper for transfer learning. The\n",
    "momentum and learning rate are too high for transfer learning. It will easily corrupt the\n",
    "pretrained weight and blow up the loss. A quick check is to see if loss (as categorical\n",
    "cross entropy) is getting significantly larger than log(NUM_CLASSES) after the same\n",
    "epoch. If so, the initial learning rate/momentum is too high.\n",
    "- Smaller batch size benefit validation accuracy, possibly due to effectively providing\n",
    "regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Using the latest EfficientNet weights\n",
    "\n",
    "Since the initial paper, the EfficientNet has been improved by various methods for data\n",
    "preprocessing and for using unlabelled data to enhance learning results. These\n",
    "improvements are relatively hard and computationally costly to reproduce, and require\n",
    "extra code; but the weights are readily available in the form of TF checkpoint files. The\n",
    "model architecture has not changed, so loading the improved checkpoints is possible.\n",
    "\n",
    "To use a checkpoint provided at\n",
    "(https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet), first\n",
    "download the checkpoint. As example, here we download noisy-student version of B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet\\\n",
    "!       /noisystudent/noisy_student_efficientnet-b1.tar.gz\n",
    "!!tar -xf noisy_student_efficientnet-b1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Then use the script efficientnet_weight_update_util.py to convert ckpt file to h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!python efficientnet_weight_update_util.py --model b1 --notop --ckpt \\\n",
    "!        efficientnet-b1/model.ckpt --o efficientnetb1_notop.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "When creating model, use the following to load new weight:\n",
    "\n",
    "```\n",
    "model = EfficientNetB0(weights=\"efficientnetb1_notop.h5\", include_top=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "image_classification_efficientnet_fine_tuning",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}