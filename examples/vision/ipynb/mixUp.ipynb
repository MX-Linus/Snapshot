{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# mixup augmentation for image classification\n",
    "\n",
    "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
    "**Date created:** 2020/03/06<br>\n",
    "**Last modified:** 2020/03/06<br>\n",
    "**Description:** How to implement mixup to augment images for image classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "mixup is a *domain-agnostic* data augmentation technique proposed in [mixup: Beyond\n",
    "Empirical Risk Minimization](https://arxiv.org/abs/1710.09412) by Zhang et al. It's\n",
    "implemented as the following -\n",
    "\n",
    "* $\\tilde{x}=\\lambda x_{i}+(1-\\lambda) x_{j}$, where $x_{i}$ and $x_{j}$ are input\n",
    "features\n",
    "* $\\bar{y}=\\lambda y_{i}+(1-\\lambda) y_{j}$, where $y_{i}$ and $y_{j}$ are one-hot\n",
    "encoded labels\n",
    "\n",
    "The technique is quite holistically named - we are literally mixing up the features and\n",
    "their corresponding labels. Implementation-wise it's simple. As you can imagine this\n",
    "recipe can be extended to a variety of data modalities such as computer vision, natural\n",
    "language processing, speech, and so on.\n",
    "\n",
    "This example requires TensorFlow 2.4 or higher, as well as TensorFlow Probability, which\n",
    "can be installed using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prepare dataset\n",
    "\n",
    "We will be using the FashionMNIST dataset for this toy example. But this same recipe can\n",
    "be used for other classification datasets as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
    "y_train = to_categorical(y_train, 10)\n",
    "\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Wrap as TensorFlow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_ds_one = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "train_ds_two = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "# As we will be mixing the images and their labels up, we\n",
    "# are combining two differently shuffled but SAME datasets.\n",
    "train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define `mix_up` Utility\n",
    "\n",
    "The authors suggests to sample $\\lambda$ from a [beta\n",
    "distribution](https://en.wikipedia.org/wiki/Beta_distribution). It should also be within\n",
    "[0, 1] range. We will now write a utility function that combines two different shuffle\n",
    "but same datasets using the mixup equations mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mix_up(ds_one, ds_two, alpha=0.2):\n",
    "    # Unpack two datasets\n",
    "    images_one, labels_one = ds_one\n",
    "    images_two, labels_two = ds_two\n",
    "    batch_size = tf.shape(images_one)[0]\n",
    "\n",
    "    # Sample lambda and reshape it to do the mixup\n",
    "    l = tfd.Beta(0.2, 0.2).sample(batch_size)\n",
    "    x_l = tf.reshape(l, (batch_size, 1, 1, 1))\n",
    "    y_l = tf.reshape(l, (batch_size, 1))\n",
    "\n",
    "    # Perform mixup on both images and labels\n",
    "    images = images_one * x_l + images_two * (1 - x_l)\n",
    "    labels = labels_one * y_l + labels_two * (1 - y_l)\n",
    "    return (images, labels)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Note** that here removing the vectorization part, we are combining two images to create\n",
    "a single one. Theoretically, we can combine as many we want but that comes at an\n",
    "increased computation cost. In certain cases, it may not help improve the performance as\n",
    "well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualize new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# First create the new dataset using our `mix_up` utility\n",
    "train_ds_mu = train_ds.map(\n",
    "    lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2), num_parallel_calls=AUTO\n",
    ")\n",
    "\n",
    "# Now, we are ready to visualize\n",
    "sample_images, sample_labels = next(iter(train_ds_mu))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, (image, label) in enumerate(zip(sample_images[:9], sample_labels[:9])):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image.numpy().squeeze())\n",
    "    print(label.numpy().tolist())\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_training_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(L.Conv2D(16, (5, 5), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "    model.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(L.Conv2D(32, (5, 5), activation=\"relu\"))\n",
    "    model.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(L.Dropout(0.2))\n",
    "    model.add(L.GlobalAvgPool2D())\n",
    "    model.add(L.Dense(128, activation=\"relu\"))\n",
    "    model.add(L.Dense(10, activation=\"softmax\"))\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "For the sake of reproducibility, we serialize the initial random weights of our shallow\n",
    "network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "initial_model = get_training_model()\n",
    "initial_model.save_weights(\"initial_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## #1 Train model with a mixed up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = get_training_model()\n",
    "model.load_weights(\"initial_weights.h5\")\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_ds_mu, validation_data=test_ds, epochs=EPOCHS)\n",
    "_, test_acc = model.evaluate(test_ds)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## #2 Train model *without* a mixed up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = get_training_model()\n",
    "model.load_weights(\"initial_weights.h5\")\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# Notice that we are NOT using the mixed up dataset here\n",
    "model.fit(train_ds_one, validation_data=test_ds, epochs=EPOCHS)\n",
    "_, test_acc = model.evaluate(test_ds)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Readers are encouraged to try out mixup on different datasets from different domains and\n",
    "experiment with the $\\lambda$ parameter. You are strongly advised to check out the\n",
    "original paper as well. In it, the authors present several ablation studies on mixup\n",
    "showing how it can improve generalization. Along with that, they also present results\n",
    "when you combine more than two images to create a single one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Notes\n",
    "\n",
    "* mixup can be extremely useful for low data regimes.\n",
    "* Label-smoothing and mixup usually do not complement each other.\n",
    "* mixup does not play well when you are using [Supervised Contrastive\n",
    "Learning](https://arxiv.org/abs/2004.11362).\n",
    "* There are a number of data augmentation techniques that expand upon mixup such as\n",
    "[CutMix](https://arxiv.org/abs/1905.04899), [AugMix](https://arxiv.org/abs/1912.02781),\n",
    "and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Reference:\n",
    "* [How to do mixup training from image files in\n",
    "Keras](https://www.dlology.com/blog/how-to-do-mixup-training-from-image-files-in-keras/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mixUp",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}