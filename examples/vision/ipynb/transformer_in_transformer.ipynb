{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Image classification with TNT(Transformer in Transformer)\n",
    "\n",
    "**Author:** [ZhiYong Chang](https://github.com/czy00000)<br>\n",
    "**Date created:** 2021/10/25<br>\n",
    "**Last modified:** 2021/11/29<br>\n",
    "**Description:** Implementing the Transformer in Transformer (TNT) model for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "This example implements the [TNT](https://arxiv.org/abs/2103.00112)\n",
    "model for image classification, and demonstrates it's performance on the CIFAR-100\n",
    "dataset.\n",
    "To keep training time reasonable, we will train and test a smaller model than is in the\n",
    "paper(0.66M params vs 23.8M params).\n",
    "TNT is a novel model for modeling both patch-level and pixel-level\n",
    "representation. In each TNT block, an ***outer*** transformer block is utilized to process\n",
    "patch embeddings, and an ***inner***\n",
    "transformer block extracts local features from pixel embeddings. The pixel-level\n",
    "feature is projected to the space of patch embedding by a linear transformation layer\n",
    "and then added into the patch.\n",
    "This example requires TensorFlow 2.5 or higher, as well as\n",
    "[TensorFlow Addons](https://www.tensorflow.org/addons/overview) package for the\n",
    "AdamW optimizer.\n",
    "Tensorflow Addons can be installed using the following command:\n",
    "```python\n",
    "pip install -U tensorflow-addons\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from itertools import repeat\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "weight_decay = 0.0002\n",
    "learning_rate = 0.001\n",
    "label_smoothing = 0.1\n",
    "validation_split = 0.2\n",
    "batch_size = 128\n",
    "image_size = (96, 96)  # resize images to this size\n",
    "patch_size = (8, 8)\n",
    "num_epochs = 50\n",
    "outer_block_embedding_dim = 64\n",
    "inner_block_embedding_dim = 32\n",
    "num_transformer_layer = 5\n",
    "outer_block_num_heads = 4\n",
    "inner_block_num_heads = 2\n",
    "mlp_ratio = 4\n",
    "attention_dropout = 0.5\n",
    "projection_dropout = 0.5\n",
    "first_stride = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_augmentation(inputs):\n",
    "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
    "    x = layers.Resizing(image_size[0], image_size[1])(x)\n",
    "    x = layers.RandomFlip(\"horizontal\")(x)\n",
    "    x = layers.RandomRotation(factor=0.1)(x)\n",
    "    x = layers.RandomContrast(factor=0.1)(x)\n",
    "    x = layers.RandomZoom(height_factor=0.2, width_factor=0.2)(x)\n",
    "    return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implement the pixel embedding and patch embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "def pixel_embed(x, image_size=image_size, patch_size=patch_size, in_dim=48, stride=4):\n",
    "    _, channel, height, width = x.shape\n",
    "    num_patches = (image_size[0] // patch_size[0]) * (image_size[1] // patch_size[1])\n",
    "    inner_patch_size = [math.ceil(ps / stride) for ps in patch_size]\n",
    "    x = layers.Conv2D(in_dim, kernel_size=7, strides=stride, padding=\"same\")(x)\n",
    "    # pixel extraction\n",
    "    x = tf.image.extract_patches(\n",
    "        images=x,\n",
    "        sizes=(1, inner_patch_size[0], inner_patch_size[1], 1),\n",
    "        strides=(1, inner_patch_size[0], inner_patch_size[1], 1),\n",
    "        rates=(1, 1, 1, 1),\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    x = tf.reshape(x, shape=(-1, inner_patch_size[0] * inner_patch_size[1], in_dim))\n",
    "    x = PatchEncoder(inner_patch_size[0] * inner_patch_size[1], in_dim)(x)\n",
    "    return x, num_patches, inner_patch_size\n",
    "\n",
    "\n",
    "def patch_embed(\n",
    "    pixel_embedding,\n",
    "    num_patches,\n",
    "    outer_block_embedding_dim,\n",
    "    inner_block_embedding_dim,\n",
    "    num_pixels,\n",
    "):\n",
    "    patch_embedding = tf.reshape(\n",
    "        pixel_embedding, shape=(-1, num_patches, inner_block_embedding_dim * num_pixels)\n",
    "    )\n",
    "    patch_embedding = layers.LayerNormalization(epsilon=1e-5)(patch_embedding)\n",
    "    patch_embedding = layers.Dense(outer_block_embedding_dim)(patch_embedding)\n",
    "    patch_embedding = layers.LayerNormalization(epsilon=1e-5)(patch_embedding)\n",
    "    patch_embedding = PatchEncoder(num_patches, outer_block_embedding_dim)(\n",
    "        patch_embedding\n",
    "    )\n",
    "    patch_embedding = layers.Dropout(projection_dropout)(patch_embedding)\n",
    "    return patch_embedding\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implement the MLP block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mlp(x, hidden_dim, output_dim, drop_rate=0.2):\n",
    "    x = layers.Dense(hidden_dim, activation=tf.nn.gelu)(x)\n",
    "    x = layers.Dropout(drop_rate)(x)\n",
    "    x = layers.Dense(output_dim)(x)\n",
    "    x = layers.Dropout(drop_rate)(x)\n",
    "    return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implement the TNT block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def transformer_in_transformer_block(\n",
    "    pixel_embedding,\n",
    "    patch_embedding,\n",
    "    out_embedding_dim,\n",
    "    in_embedding_dim,\n",
    "    num_pixels,\n",
    "    out_num_heads,\n",
    "    in_num_heads,\n",
    "    mlp_ratio,\n",
    "    attention_dropout,\n",
    "    projection_dropout,\n",
    "):\n",
    "    # inner transformer block\n",
    "    residual_in_1 = pixel_embedding\n",
    "    pixel_embedding = layers.LayerNormalization(epsilon=1e-5)(pixel_embedding)\n",
    "    pixel_embedding = layers.MultiHeadAttention(\n",
    "        num_heads=in_num_heads, key_dim=in_embedding_dim, dropout=attention_dropout\n",
    "    )(pixel_embedding, pixel_embedding)\n",
    "    pixel_embedding = layers.add([pixel_embedding, residual_in_1])\n",
    "    residual_in_2 = pixel_embedding\n",
    "    pixel_embedding = layers.LayerNormalization(epsilon=1e-5)(pixel_embedding)\n",
    "    pixel_embedding = mlp(\n",
    "        pixel_embedding, in_embedding_dim * mlp_ratio, in_embedding_dim\n",
    "    )\n",
    "    pixel_embedding = layers.add([pixel_embedding, residual_in_2])\n",
    "\n",
    "    # outer transformer block\n",
    "    _, num_patches, channel = patch_embedding.shape\n",
    "    # fuse local and global information\n",
    "    fused_embedding = tf.reshape(\n",
    "        pixel_embedding, shape=(-1, num_patches, in_embedding_dim * num_pixels)\n",
    "    )\n",
    "    fused_embedding = layers.LayerNormalization(epsilon=1e-5)(fused_embedding)\n",
    "    fused_embedding = layers.Dense(out_embedding_dim)(fused_embedding)\n",
    "    patch_embedding = layers.add([patch_embedding, fused_embedding])\n",
    "    residual_out_1 = patch_embedding\n",
    "    patch_embedding = layers.LayerNormalization(epsilon=1e-5)(patch_embedding)\n",
    "    patch_embedding = layers.MultiHeadAttention(\n",
    "        num_heads=out_num_heads, key_dim=out_embedding_dim, dropout=attention_dropout\n",
    "    )(patch_embedding, patch_embedding)\n",
    "    patch_embedding = layers.add([patch_embedding, residual_out_1])\n",
    "    residual_out_2 = patch_embedding\n",
    "    patch_embedding = layers.LayerNormalization(epsilon=1e-5)(patch_embedding)\n",
    "    patch_embedding = mlp(\n",
    "        patch_embedding, out_embedding_dim * mlp_ratio, out_embedding_dim\n",
    "    )\n",
    "    patch_embedding = layers.add([patch_embedding, residual_out_2])\n",
    "    return pixel_embedding, patch_embedding\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implement the TNT model\n",
    "The TNT model consists of multiple TNT blocks.\n",
    "In the TNT block, there are two transformer blocks where\n",
    "the outer transformer block models the global relation among patch embeddings,\n",
    "and the inner one extracts local structure information of pixel embeddings.\n",
    "The local information is added on the patch\n",
    "embedding by linearly projecting the pixel embeddings into the space of patch embedding.\n",
    "Patch-level and pixel-level position embeddings are introduced in order to\n",
    "retain spatial information. In orginal paper, the authors use the class token for\n",
    "classification.\n",
    "We use the `layers.GlobalAvgPool1D` to fuse patch information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    outer_block_embedding_dim=outer_block_embedding_dim,\n",
    "    inner_block_embedding_dim=inner_block_embedding_dim,\n",
    "    num_transformer_layer=num_transformer_layer,\n",
    "    outer_block_num_heads=outer_block_num_heads,\n",
    "    inner_block_num_heads=inner_block_num_heads,\n",
    "    mlp_ratio=mlp_ratio,\n",
    "    attention_dropout=attention_dropout,\n",
    "    projection_dropout=projection_dropout,\n",
    "    first_stride=first_stride,\n",
    "):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Image augment\n",
    "    x = data_augmentation(inputs)\n",
    "    # extract pixel embedding\n",
    "    pixel_embedding, num_patches, inner_patch_size = pixel_embed(\n",
    "        x, image_size, patch_size, inner_block_embedding_dim, first_stride\n",
    "    )\n",
    "    num_pixels = inner_patch_size[0] * inner_patch_size[1]\n",
    "    # extract patch embedding\n",
    "    patch_embedding = patch_embed(\n",
    "        pixel_embedding,\n",
    "        num_patches,\n",
    "        outer_block_embedding_dim,\n",
    "        inner_block_embedding_dim,\n",
    "        num_pixels,\n",
    "    )\n",
    "    # create multiple layers of the TNT block.\n",
    "    for _ in range(num_transformer_layer):\n",
    "        pixel_embedding, patch_embedding = transformer_in_transformer_block(\n",
    "            pixel_embedding,\n",
    "            patch_embedding,\n",
    "            outer_block_embedding_dim,\n",
    "            inner_block_embedding_dim,\n",
    "            num_pixels,\n",
    "            outer_block_num_heads,\n",
    "            inner_block_num_heads,\n",
    "            mlp_ratio,\n",
    "            attention_dropout,\n",
    "            projection_dropout,\n",
    "        )\n",
    "    patch_embedding = layers.LayerNormalization(epsilon=1e-5)(patch_embedding)\n",
    "    x = layers.GlobalAvgPool1D()(patch_embedding)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train on CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
    "    optimizer=tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    ),\n",
    "    metrics=[\n",
    "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_split=validation_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Visualize the training progress of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train and Validation Accuracies Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Let's display the final results of the test on CIFAR-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {round(loss, 2)}\")\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "After 50 epochs, the TNT model achieves around 42% accuracy and\n",
    "73% top-5 accuracy on the test data. It only has 0.6M parameters.\n",
    "From the above loss curve, we can find that the model gradually converges,\n",
    "but it never achieves state of the art performance. We could apply further data\n",
    "augmentation to\n",
    "obtain better performance, like [RandAugment](https://arxiv.org/abs/1909.13719),\n",
    "[MixUp](https://arxiv.org/abs/1710.09412)\n",
    "etc. We also can adjust the depth of model, learning rate or increase the size of\n",
    "embedding. Compared to the conventional\n",
    "vision transformers [ViT](https://arxiv.org/abs/2010.11929) which corrupts the local\n",
    "structure\n",
    "of the patch, the TNT can better preserve and model the local information\n",
    "for visual recognition."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_in_transformer",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}