{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Text Image Denoiser.\n",
    "\n",
    "**Author:** [Anish B](https://twitter.com/anishhacko)<br>\n",
    "**Date created:** 2021/01/18<br>\n",
    "**Last modified:** 2022/01/27<br>\n",
    "**Description:** Example of Text Image Denoiser coupled with Tesseract OCR engine to improve text readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This Example explains a simple **Text Image Denoiser for OCR** using U-Net based\n",
    "Architectutre. **Autoencoders** are mostly employed for **Image Restoration** problems\n",
    "but in recent times **U-Net** based architectures with **skip-connections** have gained\n",
    "popularity for several Image-to-Image tasks. We aim to solve the problem with simple\n",
    "**Pretrained U-Net as Encoder** and **Efficient Sub-Pixel CNN as Decoder**. The problem\n",
    "we focus here is enhancing the Document Images which are deteriorated through external\n",
    "degradations like blur, corrupt text blocks etc. We have followed up on few seminal\n",
    "papers which are presented below for reference. At the end of this tutorial user will\n",
    "gain clear understanding of building Custom Image Pre-Processors using Deep-Learning that\n",
    "helps us to mitigate real world OCR issues.\n",
    "\n",
    "**References:**\n",
    "- [Enhancing OCR Accuracy with Super\n",
    "Resolution](https://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2018/ocr_Ankit_Lat_ICPR_2018.pdf)\n",
    "- [Improving the Perceptual Quality of Document\n",
    "Images Using Deep Neural\n",
    "Network](http://mile.ee.iisc.ac.in/publications/softCopy/DocumentAnalysis/ISNN_11page_65.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Additional Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# !pip install pybind11\n",
    "# !pip install fastwer\n",
    "# !pip install pytesseract\n",
    "# !sudo apt install tesseract-ocr\n",
    "\n",
    "import fastwer\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset\n",
    "We have created a synthetic document dataset that depicts several real world document\n",
    "noises; we have used Blurs and Morphological filters to reconstruct real world noises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!gdown https://drive.google.com/uc?id=1jQfHJUUQcpktQcjIzitvb4SldKwqcAMr\n",
    "!!unzip -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_source = sorted(glob(\"data/source/*.jpg\"))[:-80]\n",
    "train_targets = sorted(glob(\"data/target/*.jpg\"))[:-80]\n",
    "\n",
    "val_source = sorted(glob(\"data/source/*.jpg\"))[-80:]\n",
    "val_targets = sorted(glob(\"data/target/*.jpg\"))[-80:]\n",
    "\n",
    "HEIGHT, WIDTH = 352, 608\n",
    "BATCH_SIZE = 4\n",
    "BUFFER_SIZE = int(len(train_source) * 0.2)\n",
    "\n",
    "\n",
    "def resize_with_pad(image, height, width):\n",
    "    return tf.image.resize_with_pad(\n",
    "        image=image, target_height=height, target_width=width\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    image = tf.io.decode_png(tf.io.read_file(image), channels=3)\n",
    "    image = resize_with_pad(image, height=HEIGHT, width=WIDTH)\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def data_preprocess(source, target):\n",
    "    source_image = preprocess(source)\n",
    "    target_image = preprocess(target)\n",
    "    return source_image, target_image\n",
    "\n",
    "\n",
    "def denormalize(array):\n",
    "    array += 1\n",
    "    array *= 127.5\n",
    "    return array\n",
    "\n",
    "\n",
    "train_pipe = tf.data.Dataset.from_tensor_slices((train_source, train_targets))\n",
    "train_pipe = train_pipe.map(data_preprocess, tf.data.AUTOTUNE).shuffle(BUFFER_SIZE)\n",
    "train_set = train_pipe.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_pipe = tf.data.Dataset.from_tensor_slices((val_source, val_targets))\n",
    "valid_set = val_pipe.map(data_preprocess, tf.data.AUTOTUNE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset Visualiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax = ax.flatten()\n",
    "for i, f in enumerate(valid_set.take(1)):\n",
    "    noise, clean = f\n",
    "    noise = denormalize(noise[i].numpy())\n",
    "    clean = denormalize(clean[i].numpy())\n",
    "    ax[0].imshow(noise.astype(np.int))\n",
    "    ax[0].set_xlabel(\"source\")\n",
    "    ax[1].imshow(clean.astype(np.int))\n",
    "    ax[1].set_xlabel(\"target\")\n",
    "fig.text(x=0.45, y=0.78, s=\"Dataset Visualization\", fontweight=\"bold\")\n",
    "fig.set_tight_layout(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model\n",
    "\n",
    "We have used Pre-trained ```ResNet50V2``` U-Net as Encoder, The Feature Maps are\n",
    "extracted at different levels forming Downscaling path, Then ```Sub-Pixel Layers``` on\n",
    "the Upscaling path with skip-connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv_unit(x, filters=64, padding=\"same\"):\n",
    "    x = keras.layers.Conv2D(filters=filters, kernel_size=3, padding=padding)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def subpixel_upsample(x, target_channels, upscale_ratio):\n",
    "    estimated_filters = target_channels * (upscale_ratio ** 2)\n",
    "    x = keras.layers.Conv2D(estimated_filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU(0.2)(x)\n",
    "    x = tf.nn.depth_to_space(x, block_size=upscale_ratio)\n",
    "    return x\n",
    "\n",
    "\n",
    "def upscale_unit(previous_input, current_input, target_channels=3, upscale_ratio=2):\n",
    "    previous_low_scale = subpixel_upsample(\n",
    "        previous_input, target_channels=target_channels, upscale_ratio=upscale_ratio\n",
    "    )\n",
    "    current_high_scale = conv_unit(current_input, filters=target_channels)\n",
    "    concat_scales = tf.concat((previous_low_scale, current_high_scale), axis=-1)\n",
    "    feature_manifold = conv_unit(concat_scales, filters=target_channels)\n",
    "    return feature_manifold\n",
    "\n",
    "\n",
    "def denoiser():\n",
    "    base_model = ResNet50V2(\n",
    "        include_top=False, weights=\"imagenet\", input_shape=(352, 608, 3)\n",
    "    )\n",
    "    encode0 = base_model.get_layer(\"input_1\").output  # (None,352,608,3)\n",
    "    encode1 = base_model.get_layer(\"conv1_conv\").output  # (None,176,304,64)\n",
    "    encode2 = base_model.get_layer(\"conv2_block3_1_relu\").output  # (None,88,152,64)\n",
    "    encode3 = base_model.get_layer(\"conv3_block4_1_relu\").output  # (None,44,76,128)\n",
    "\n",
    "    decode1 = upscale_unit(\n",
    "        previous_input=encode3, current_input=encode2, target_channels=128\n",
    "    )\n",
    "    decode2 = upscale_unit(\n",
    "        previous_input=decode1, current_input=encode1, target_channels=64\n",
    "    )\n",
    "    decode3 = upscale_unit(\n",
    "        previous_input=decode2, current_input=encode0, target_channels=32\n",
    "    )\n",
    "\n",
    "    out_conv = keras.layers.Conv2D(3, 5, padding=\"same\", activation=\"sigmoid\")(decode3)\n",
    "    denoiser_model = keras.Model(base_model.input, out_conv, name=\"denoiser\")\n",
    "    denoiser_model.summary()\n",
    "    return denoiser_model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training\n",
    "\n",
    "The Model is Trained with an Objective of minimizing ```Mean Squared Error```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "denoiser_net = denoiser()\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=8)\n",
    "ckpt = callbacks.ModelCheckpoint(\n",
    "    \"best_ckpt.h5\", save_best_only=True, save_weights_only=True, verbose=1\n",
    ")\n",
    "optimizer = keras.optimizers.Adam(LEARNING_RATE)\n",
    "denoiser_net.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "denoiser_net.fit(\n",
    "    train_set,\n",
    "    validation_data=valid_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_source) // BATCH_SIZE,\n",
    "    validation_steps=len(val_source) // BATCH_SIZE,\n",
    "    workers=-1,\n",
    "    callbacks=[early_stop, ckpt],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Evaluation Utilities\n",
    "\n",
    "Image Super-Resolution or Denoiser models can be evaluated with PSNR, SSIM metrics for\n",
    "perceptual quality assessments but our task of **Text Restoration** requires a bit more,\n",
    "our objective is to Enhance simple OCR accuracy engine where our model can act as a\n",
    "**Pre-Processor**, to serve the purpose we will use <a\n",
    "href=\"https://towardsdatascience.com/evaluating-ocr-output-quality-with-character-error-ra\n",
    "te-cer-and-word-error-rate-wer-853175297510\">WER(Word Error Rate) </a> as a metric to\n",
    "evaluate the real-world performance. The following codes will help plot the visual\n",
    "comparisons of outputs given the **Raw Noisy Input** and **Model Restored Input** to\n",
    "tesseract OCR engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def word_error_rate(noise_image, reference_image, prediction_image):\n",
    "    reference = pytesseract.image_to_string(reference_image)\n",
    "    noiseimage_ocr_out = pytesseract.image_to_string(noise_image)\n",
    "    predimage_ocr_out = pytesseract.image_to_string(prediction_image)\n",
    "    noise_WER_score = fastwer.score_sent(noiseimage_ocr_out, reference)\n",
    "    pred_WER_score = fastwer.score_sent(predimage_ocr_out, reference)\n",
    "    return noise_WER_score, pred_WER_score\n",
    "\n",
    "\n",
    "def generate_results(file):\n",
    "    \"\"\"\n",
    "    The following function generates visual and WER (Word Error Rates) comparison\n",
    "    plots between raw input image when passed through OCR engine and model reconstructed\n",
    "    image when passed through OCR, signifying the major improvement the model can offer\n",
    "    in a workflow.\n",
    "    \"\"\"\n",
    "\n",
    "    source_image = tf.io.decode_png(tf.io.read_file(file), channels=3)\n",
    "    source_image = resize_with_pad(source_image, height=HEIGHT, width=WIDTH)\n",
    "    source_image = preprocess_input(source_image)\n",
    "    noise_ground_truth = denormalize(source_image)\n",
    "    noise_ground_truth = keras.utils.array_to_img(noise_ground_truth)\n",
    "    clean_ground_truth = tf.io.decode_png(\n",
    "        tf.io.read_file(file.replace(\"source\", \"target\")), channels=3\n",
    "    )\n",
    "    clean_ground_truth = resize_with_pad(clean_ground_truth, height=HEIGHT, width=WIDTH)\n",
    "    clean_ground_truth = keras.utils.array_to_img(clean_ground_truth)\n",
    "\n",
    "    source_image = tf.expand_dims(source_image, axis=0)\n",
    "    prediction = denoiser_net.predict(source_image)\n",
    "    prediction = denormalize(prediction[0])\n",
    "    prediction = keras.utils.array_to_img(prediction)\n",
    "\n",
    "    noise_error_rate, reconstructed_error_rate = word_error_rate(\n",
    "        noise_image=noise_ground_truth,\n",
    "        reference_image=clean_ground_truth,\n",
    "        prediction_image=prediction,\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    axes[0].imshow(noise_ground_truth)\n",
    "    axes[0].set_xlabel(f\"Input-WER score : {round(noise_error_rate,2)}\")\n",
    "    axes[1].imshow(prediction)\n",
    "    axes[1].set_xlabel(f\"Reconstructed-WER score : {round(reconstructed_error_rate,2)}\")\n",
    "    #     axes[2].imshow(clean_ground_truth); axes[2].set_xlabel(\"clean_ground-truth\")\n",
    "    fig.suptitle(\n",
    "        \"Visual comparison and WER(Word Error Rate) Evaluation charts\",\n",
    "        weight=\"bold\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    return noise_ground_truth, prediction, clean_ground_truth\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "test_images = [np.random.choice(val_source) for i in range(6)]\n",
    "for f in test_images:\n",
    "    generate_results(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We are able to observe using the model as a Pre-Processor enables significant amount of\n",
    "improvement in Word Error Rate and perceptual quality of the image, the experiment can be\n",
    "extended to different Image-to-Image applications, also Training with huge Dataset and\n",
    "longer Epochs can elevate the performance of the model further."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_image_denoiser",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}