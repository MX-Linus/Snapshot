{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2h__HbR944VZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5c3b25-6238-4c32-f257-47226e86b567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 588.3 MB 22 kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 76.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 439 kB 109.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 103.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 91.2 MB/s \n",
            "\u001b[?25h  Building wheel for keras-nlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install 0.4 preview from source\n",
        "!pip install -q git+https://github.com/keras-team/keras-nlp.git tensorflow==2.11 --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rlmPYqUn7YJm"
      },
      "outputs": [],
      "source": [
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Use mixed precision for optimal performance\n",
        "keras.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# KerasNLP: Modular NLP Workflows for Keras\n",
        "\n",
        "\n",
        "KerasNLP is a natural language processing library that supports users through their entire development cycle. Our workflows are built from modular components that have SoTA preset weights and architectures when used out-of-the-box and are easily customizable when more control is needed.\n",
        "\n",
        "This library is an extension of the core Keras API; all high level modules are [`Layers`](https://keras.io/api/layers/) or\n",
        "[`Models`](https://keras.io/api/models/). If you are familiar with Keras, congratulations! You already understand most of KerasNLP.\n",
        "\n",
        "This guide demonstrates our modular approach using a sentiment analysis example at six levels of complexity:\n",
        "\n",
        "* Inference with a pretrained classifier\n",
        "* Fine tuning a pretrained backbone\n",
        "* Fine tuning with user-controlled preprocessing\n",
        "* Fine tuning a custom model\n",
        "* Pretraining a backbone model\n",
        "* Build and train your own transformer from scratch\n",
        "\n",
        "Throughout our guide we use Professor Keras, the official Keras mascot, as a visual reference for the complexity of the material:\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1d14Qpmfgjf6zu4z30HBaonH8PYDHgVoU)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7aT5DHgGCvl"
      },
      "source": [
        "# API quickstart\n",
        "\n",
        "Our highest level API is `keras_nlp.models`. These symbols cover the complete user journey of converting strings to tokens, tokens to dense features, and dense features to task-specific output. For each `XX` architecture (e.g., `Bert`), we offer the following modules:\n",
        "\n",
        "* **Tokenizer**: `keras_nlp.models.XXTokenizer`\n",
        "    * **What it does**: Converts strings to `tf.RaggedTensor`s of token ids.\n",
        "    * **Why it's important**: The raw bytes of a string are too high dimensional to be useful features so we first map them to a small number of tokens, for example `\"The quick brown fox\"` to `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`.\n",
        "    * **Inherits from**: `keras.Layer`.\n",
        "* **Preprocessor**: `keras_nlp.models.XXPreprocessor`\n",
        "    * **What it does**: Converts strings to a dictonary of preprocessed tensors consumed by the backbone, starting with tokenization.\n",
        "    * **Why it's important**: Each model uses special tokens and extra tensors to understand the input such as deliminting input segments and identifying padding tokens. Padding each sequence to the same length improves computational efficiency.\n",
        "    * **Has a**: `XXTokenizer`.\n",
        "    * **Inherits from**: `keras.Layer`.\n",
        "* **Backbone**: `keras_nlp.models.XXBackbone`\n",
        "    * **What it does**: Converts preprocessed tensors to dense features. *Does not handle strings; call the preprocessor first.*\n",
        "    * **Why it's important**: The backbone distills the input tokens into dense features that can be used in downstream tasks. It is generally pretrained on a language modeling task using massive amounts of unlabeled data. Transfering this information to a new task is a major breakthrough in modern NLP.\n",
        "    * **Inherits from**: `keras.Model`.\n",
        "* **Task**: e.g., `keras_nlp.models.XXClassifier`\n",
        "    * **What it does**: Converts strings to task-specific output (e.g., classification probabilities).\n",
        "    * **Why it's important**: Task models combine string preprocessing and the backbone model with task-specific `Layers` to solve a problem such as sentence classification, token classification, or text generation. The additional `Layers` must be fine-tuned on labeled data.\n",
        "    * **Has a**: `XXBackbone` and `XXPreprocessor`.\n",
        "    * **Inherits from**: `keras.Model`.\n",
        "\n",
        "Here is the modular hierarchy for `BertClassifier` (all relationships are compositional):\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1vHBQ1oFbto8ItfhsLcxKhIwOIdJE1X9n)\n",
        "\n",
        "All modules can be used independently and have a `from_preset()` method in addition to the standard constructor that instantiates the class with **preset** architecture and weights (see examples below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlyeJqptmagk"
      },
      "source": [
        "# Data\n",
        "\n",
        "We will use a running example of sentiment analysis of IMDB movie reviews. In this task, we use the text to predict whether the review was positive (`label = 1`) or negative (`label = 0`).\n",
        "\n",
        "We load the data using `keras.utils.text_dataset_from_directory`, which utilizes the powerful `tf.data.Dataset` format for examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHDYfqML56Gs",
        "outputId": "52bd8f74-216f-4ed3-b5a3-518973f46b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  31.1M      0  0:00:02  0:00:02 --:--:-- 31.1M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "# Remove unsupervised examples\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3Q1wpT6mh5o",
        "outputId": "231fa12e-8793-493a-ade4-77dea49fe5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'I avoided watching this film for the longest time. Long before it was even released I had dismissed it as an over-hyped, over-blown, overly romanticized piece of Hollywood schmaltz, and I wanted nothing to do with it. I never watched it in the theatre. I shook my head in disbelief at the 11 Academy Awards - even though I had never seen it. Then I was asked to be a judge at a high school public speaking contest. One of the girls spoke about this movie. \"It was so great,\" she said. \"You really felt like you were on the ship.\" \"Nonsense,\" I thought. I shared my feelings with my fellow judges. One looked at me and said, \"you might be right, but if she liked the movie that much maybe she\\'ll want to learn more about the real Titanic. The movie must have done something right to get her so interested.\" \"Well, maybe,\" thought I. Then it finally appeared on Pay TV. \"OK,\" I thought, \"I\\'ll give it a look see.\" I didn\\'t want to like it - and I didn\\'t. I loved it! What a great movie.<br /><br />Where to start? First - the directing. My high school public speaking contestant was right. James Cameron does a superb job of creating an almost \"you are there\" type of atmosphere. The gaiety of life aboard the most elegant ship in the world. The nonchalance as news of the iceberg first spreads; then the rising sense of panic. You don\\'t just watch it; you really do feel it. Then - the performances. The lead performances from Kate Winslet (as Rose) and Leonardo DiCaprio (as Jack) are excellent - Winslet\\'s being the superior, I thought, but both were good. They had their rich girl/poor boy characters down to a perfect \"t\" I thought. In my opinion, though, stealing the show was Frances Fisher as Rose\\'s mother. She was perfect as the snobby aristocrat, and you could feel the fear and loathing she felt every time she looked at Jack. Then - the details. I\\'m no expert on the sinking of the Titanic, but I have a reasonable general knowledge, and this film does a super job of recreating the historical details accurately and then weaving them seamlessly around the fictional romance. Very impressive, indeed. Then - the song. Who can watch this movie and not be taken with Celine Dion\\'s performance of \"My Heart Goes On.\"<br /><br />Problems. Well, the romance was perhaps too contrived, in the sense that I just don\\'t accept that Jack could have moved so effortlessly from steerage to first class. (I know he was invited the first time; but he seems to keep getting into first class without being stopped until he\\'s been there for a while.) The realities of the separation of the social classes were much more realistically portrayed, I thought, when the steerage passengers were going to be left locked down there after the ship hit the iceberg while the first class folks got to enjoy half empty lifeboats. <br /><br />A minor quibble, though. This is truly an excellent movie. My only regret is not seeing it in the theatre, where I think it would have been so much more impressive.<br /><br />9/10'>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "imdb_train = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "imdb_test = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/test\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# Inspect first review\n",
        "# Format is (review text tensor, label tensor)\n",
        "print(imdb_train.unbatch().take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaecCDgL0OfA"
      },
      "source": [
        "# Inference with a pretrained classifier\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1xeMHVCxYhm3_oC37Gg7k0bG-yhsVr0Dv)\n",
        "\n",
        "The highest level module in KerasNLP is a **task**. A **task** is a `keras.Model` consisting of a (generally pretrained) **backbone** model and task-specific layers. Here's an example using `keras_nlp.models.BertClassifier`.\n",
        "\n",
        "**Note**: Outputs are the logits per class (e.g., `[0, 0]` is 50% chance of positive).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdckAMP_6nkH",
        "outputId": "232ca053-39de-4500-a42b-e094f3ec4bf4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[-1.54 ,  1.543]], dtype=float16)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n",
        "# Note: batched inputs expected so must wrap string in iterable\n",
        "classifier.predict([\"I love modular workflows in keras-nlp!\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7sz3bVw83tb"
      },
      "source": [
        "All **tasks** have a `from_preset` method that constructs a `keras.Model` instance with preset preprocessing, architecture and weights. This means that we can pass raw strings in any format accepted by a `keras.Model` and get output specific to our task.\n",
        "\n",
        "This particular **preset** is a `\"bert_tiny_uncased_en\"` **backbone** fine-tuned on `sst2`, another movie review sentiment analysis (this time from Rotten Tomatoes). We use the `tiny` architecture for demo purposes, but larger models are recommended for SoTA performance. For all the task-specific presets available for `BertClassifier`, see [keras.io](https://resilient-dango-43f7b8.netlify.app/api/keras_nlp/models/).\n",
        "\n",
        "Let's evaluate our classifier on the IMDB dataset. We first need to compile the `keras.Model`. The output is `[loss, accuracy]`,\n",
        "\n",
        "**Note**: We don't need an optimizer since we're not training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD-NOMU-CG_8",
        "outputId": "2d27fedd-fc5f-4699-ebfa-6e26610456b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 50s 30ms/step - loss: 0.4630 - sparse_categorical_accuracy: 0.7836\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.4629538357257843, 0.7836400270462036]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "\n",
        "classifier.evaluate(imdb_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1xNX2w97cL6"
      },
      "source": [
        "Our result is 78% accuracy without training anything. Not bad!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSea_qw0_UH5"
      },
      "source": [
        "# Fine tuning a pretrained BERT backbone\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1YytOYRSqsrhJ4NLatVOSuVMbLPa9iXrw)\n",
        "\n",
        "When labeled text specific to our task is available, fine-tuning a custom classifier can improve performance. If we want to predict IMDB review sentiment, using IMDB data should perform better than Rotten Tomatoes data! And for many tasks no relevant pretrained model will be available (e.g., categorizing customer reviews).\n",
        "\n",
        "The workflow for fine-tuning is almost identical to above, except that we request a **preset** for the **backbone**-only model rather than the entire classifier. When passed a **backone** **preset**, a **task** `Model` will randomly initialize all task-specific layers in preparation for training. For all the **backbone** presets available for `BertClassifier`, see [keras.io](https://resilient-dango-43f7b8.netlify.app/api/keras_nlp/models/).\n",
        "\n",
        "To train your classifier, use `Model.compile()` and `Model.fit()` as with any other `keras.Model`. Since preprocessing is included in all **tasks** by default, we again pass the raw data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHAPoLgL_7Se",
        "outputId": "8e8542f1-14b9-4a12-c5ed-ec3ea7d76f19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 301s 182ms/step - loss: 0.4116 - sparse_categorical_accuracy: 0.8052 - val_loss: 0.3101 - val_sparse_categorical_accuracy: 0.8685\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff102274ca0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    num_classes=2,\n",
        ")\n",
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.experimental.AdamW(5e-5),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "classifier.fit(\n",
        "    imdb_train,\n",
        "    validation_data=imdb_test,\n",
        "    epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRz37_8Gu1gQ"
      },
      "source": [
        "Here we see significant lift in validation accuracy (0.78 -> 0.87) with a single epoch of training even though the IMDB dataset is much smaller than `sst2`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgTttBkcCvb2"
      },
      "source": [
        "# Fine tuning with user-controlled preprocessing\n",
        "![picture](https://drive.google.com/uc?id=1T_40vtl8daihS-kKYTFWejFd19KJAyDK)\n",
        "\n",
        "For some advanced training scenarios, users might prefer direct control over preprocessing. For large datasets, examples can be preprocessed in advance and saved to disk or preprocessed by a separate worker pool using `tf.data.experimental.service`. In other cases, custom preprocessing is needed to handle the inputs.\n",
        "\n",
        "Pass `preprocessor=None` to the constructor of a **task** `Model` to skip automatic preprocessing or pass a custom `BertPreprocessor` instead.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_fkpErSkyCk"
      },
      "source": [
        "## Separate preprocessing from the same preset\n",
        "\n",
        "Each model architecture has a parallel **preprocessor** `Layer` with its own `from_preset` constructor. Using the same **preset** for this `Layer` will return the matching **preprocessor** as the **task**.\n",
        "\n",
        "In this workflow we train the model over three epochs using `tf.data.Dataset.cache()`, which computes the preprocessing once and caches the result before fitting begins.\n",
        "\n",
        "**Note:** this code only works if your data fits in memory. If not, pass a `filename` to `cache()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYjpZzRSDmmv",
        "outputId": "786f2bed-8a7e-42e6-91e0-70984237f36d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_tiny_en_uncased/v1/model.h5\n",
            "17602216/17602216 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 277s 166ms/step - loss: 0.4129 - sparse_categorical_accuracy: 0.8084 - val_loss: 0.3361 - val_sparse_categorical_accuracy: 0.8585\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 238s 152ms/step - loss: 0.2640 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.3277 - val_sparse_categorical_accuracy: 0.8689\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 237s 152ms/step - loss: 0.1954 - sparse_categorical_accuracy: 0.9282 - val_loss: 0.3837 - val_sparse_categorical_accuracy: 0.8631\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdcead2e610>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    sequence_length=512,\n",
        ")\n",
        "\n",
        "imdb_train_cached = imdb_train.map(\n",
        "    preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "imdb_test_cached = imdb_test.map(\n",
        "    preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    preprocessor=None,\n",
        ")\n",
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.experimental.AdamW(5e-5),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "classifier.fit(\n",
        "    imdb_train_cached,\n",
        "    validation_data=imdb_test_cached,\n",
        "    epochs=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T1qBvAcyhfj"
      },
      "source": [
        "After three epochs, our validation accuracy has only increased to 0.88. This is both a function of the small size of our dataset and our model. To exceed 90% accuracy, try larger **presets** such as  `\"bert_base_en_uncased\"`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEU576ooLa15"
      },
      "source": [
        "## Custom preprocessing\n",
        "\n",
        "In cases where custom preprocessing is required, we offer direct access to the `Tokenizer` class that maps raw strings to tokens. It also has a `from_preset` constructor to get the vocabulary matching pretraining.\n",
        "\n",
        "**Note:** `BertTokenizer` does not pad sequences by default, so the output is `tf.RaggedTensor`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8FbsYprLZ7V",
        "outputId": "fb8eb51b-9dd9-450f-95c9-16922621907e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[1045, 2293, 19160, 2147, 12314, 2015, 999],\n",
              " [8860, 2058, 7705, 2015, 999]]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_tiny_en_uncased\")\n",
        "tokenizer([\"I love modular workflows!\", \"Libraries over frameworks!\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGvck1j-MZ_M",
        "outputId": "62653976-65de-44d2-be54-cb07022acacc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'token_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
            "array([  101,  2130,  2065,  2009,  2020, 19512,  6057,  1010,  2023,\n",
            "        9587, 21285,  2100, 13844,  6198,  1997,  1037,  2143,  2052,\n",
            "        2145,  2022, 17358, 15787,  4487, 21338,  2229,  5051,  6593,\n",
            "        3993,  1012,  7458,  2074,  2460,  1997, 10443,  2039,  1996,\n",
            "        3337,  1005, 18113,  1998,  2128,  1011,  4372, 18908,  2075,\n",
            "        1005,  5353,  2012, 15941,  1005,  1055,  1005,  2021,  2069,\n",
            "        2074,  3135,  6554, 25546,  1998,  1996,  2472,  1997,  1996,\n",
            "         102], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
            "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "      dtype=int32)>, 'padding_mask': <tf.Tensor: shape=(64,), dtype=bool, numpy=\n",
            "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True])>}, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ]
        }
      ],
      "source": [
        "# Write your own packer or use one our `Layers`\n",
        "packer = keras_nlp.layers.MultiSegmentPacker(\n",
        "    start_value=tokenizer.cls_token_id,\n",
        "    end_value=tokenizer.sep_token_id,\n",
        "    # Note: This cannot be longer than the preset's `sequence_length`, and there\n",
        "    # is no check for a custom preprocessor!\n",
        "    sequence_length=64,\n",
        ")\n",
        "\n",
        "def preprocessor(x, y):\n",
        "    token_ids, segment_ids = packer(tokenizer(x))\n",
        "    x = {\n",
        "        \"token_ids\": token_ids,\n",
        "        \"segment_ids\": segment_ids,\n",
        "        \"padding_mask\": token_ids != 0,\n",
        "    }\n",
        "    return x, y\n",
        "\n",
        "imbd_train_preprocessed = imdb_train.map(\n",
        "    preprocessor, tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "imdb_test_preprocessed = imdb_test.map(\n",
        "    preprocessor, tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Preprocessed example\n",
        "print(imbd_train_preprocessed.unbatch().take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kF05VQIJFMo"
      },
      "source": [
        "# Fine tuning with a custom model\n",
        "![picture](https://drive.google.com/uc?id=1T_40vtl8daihS-kKYTFWejFd19KJAyDK)\n",
        "\n",
        "For more advanced applications, an appropriate **task** `Model` may not be available. In this case we provide direct access to the **backbone** `Model`, which has its own `from_preset` constructor and can be composed with custom `Layer`s. Detailed examples can be found at https://keras.io/guides/transfer_learning/.\n",
        "\n",
        "A **backbone** `Model` does not include automatic preprocessing but can be paired with a matching **preprocessor** using the same **preset** as shown in the previous workflow.\n",
        "\n",
        "In this workflow we experiment with freezing our backbone model and adding two trainable transfomer layers to adapt to the new input.\n",
        "\n",
        "**Note**: We can igonore the warning about gradients for the `pooled_dense` layer because we are using BERT's sequence output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yivypQjiRdi7",
        "outputId": "db768a2a-852b-405a-b295-d7f35204a0e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " padding_mask (InputLayer)      [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " segment_ids (InputLayer)       [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " token_ids (InputLayer)         [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " bert_backbone_3 (BertBackbone)  {'sequence_output':  4385920    ['padding_mask[0][0]',           \n",
            "                                 (None, None, 128),               'segment_ids[0][0]',            \n",
            "                                 'pooled_output': (               'token_ids[0][0]']              \n",
            "                                None, 128)}                                                       \n",
            "                                                                                                  \n",
            " transformer_encoder (Transform  (None, None, 128)   198272      ['bert_backbone_3[0][1]']        \n",
            " erEncoder)                                                                                       \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Transfo  (None, None, 128)   198272      ['transformer_encoder[0][0]']    \n",
            " rmerEncoder)                                                                                     \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_4 (Sl  (None, 128)         0           ['transformer_encoder_1[0][0]']  \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 2)            258         ['tf.__operators__.getitem_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,782,722\n",
            "Trainable params: 396,802\n",
            "Non-trainable params: 4,385,920\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 60s 28ms/step - loss: 0.5779 - sparse_categorical_accuracy: 0.6948 - val_loss: 0.5332 - val_sparse_categorical_accuracy: 0.7333\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.4830 - sparse_categorical_accuracy: 0.7662 - val_loss: 0.4282 - val_sparse_categorical_accuracy: 0.8007\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.4351 - sparse_categorical_accuracy: 0.7998 - val_loss: 0.4045 - val_sparse_categorical_accuracy: 0.8171\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff0c859fe80>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_tiny_en_uncased\")\n",
        "backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_tiny_en_uncased\")\n",
        "\n",
        "imdb_train_preprocessed = imdb_train.map(\n",
        "    preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "imdb_test_preprocessed = imdb_test.map(\n",
        "    preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "backbone.trainable = False\n",
        "inputs = backbone.input\n",
        "sequence = backbone(inputs)[\"sequence_output\"]\n",
        "for _ in range(2):\n",
        "  sequence = keras_nlp.layers.TransformerEncoder(\n",
        "      num_heads=2,\n",
        "      intermediate_dim=512,\n",
        "      dropout=0.1,\n",
        "  )(sequence)\n",
        "# Use [CLS] token output to classify\n",
        "outputs = keras.layers.Dense(2)(sequence[:, backbone.cls_token_index, :])\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.experimental.AdamW(5e-5),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "model.summary()\n",
        "model.fit(\n",
        "    imdb_train_preprocessed,\n",
        "    validation_data=imdb_test_preprocessed,\n",
        "    epochs=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnYHsiQYEDY8"
      },
      "source": [
        "This model achieves reasonable accuracy despite having only 10% the trainable parameters of our `BertClassifier` model. Each training step takes about 1/3 of the time---even accounting for cached preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HcdM0yuTKpE"
      },
      "source": [
        "# Pretraining a backbone model\n",
        "![picture](https://drive.google.com/uc?id=1pzwLPCtvzmHY3DKzH-MBzmjWFJ3pKVB5)\n",
        "\n",
        "Do you have access to large unlabeled datasets in your domain? Are they are around the same size as used to train popular backbones such as BERT, RoBERTa, or GPT2 (XX+ GiB)? If so, you might benefit from domain-specific pretraining of your own backbone models.\n",
        "\n",
        "NLP models are generally pretrained on a language modeling task, predicting masked words given the visible words in an input sentence. For example, given the input `\"The fox [MASK] over the [MASK] dog\"`, the model might be asked to predict `[\"jumped\", \"lazy\"]`. The lower layers of this model are then packaged as a **backbone** to be combined with layers relating to a new task.\n",
        "\n",
        "The KerasNLP library offers SoTA **backbones** and **tokenizers** to be trained from scratch without presets.\n",
        "\n",
        "In this workflow we pretrain a BERT **backbone** using our IMDB review text. We skip the \"next sentence prediction\" (NSP) loss because it adds significant complexity to the data processing and was dropped by later models like RoBERTa. See our e2e [BERT pretraining example](https://github.com/keras-team/keras-nlp/tree/4f9ebefa82af22b4f4267dfa80fa525f7a03bd5d/examples/bert) for step-by-step details on how to replicate the original paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tERSi-TnDKOx"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cad9cIrGTIPP",
        "outputId": "e04a0c6b-e9c3-461b-f9ef-f16dee86332c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'token_ids': <tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
            "array([  101,  2702,  2041,  1997,   103,  2340,  2460,  3152,  1999,\n",
            "        2023,  3185,  2024, 17743,  8815,   103,  1045,   103,  2069,\n",
            "         103,   103,  2028, 15640,   103,  1012,  2358, 24449,  2135,\n",
            "        1010,  2035,   103,  1996,   103,  2472,  4900,  2000, 17279,\n",
            "        1996,   103,  1997,  3633,  2030,  2967,  1999,  4434,  2007,\n",
            "        1023,  1011,  2340,  1024,  1996,   103,  8711,  1010, 12419,\n",
            "         103,  1010, 21524,   103,  1996, 24835,  1997,  5034, 15878,\n",
            "        7389,  5555,  1010,   103,   103,  5635,  1998,  7897,  1999,\n",
            "        3088,  1010,   103, 23555,  3215,  8648,  1998, 13831,   103,\n",
            "         103,  2232,  1010,   103,   103,  1999,  3956,  1010, 27890,\n",
            "        1011,  2718,  1998,  2110,  1011, 27666,  5152,  4841,  1999,\n",
            "        1996,  3915,  1010,  2214,  2111,  2542,  2894,   103,   103,\n",
            "         103, 10530,   103, 25755,  1999,  1996,  8072,  1997,   103,\n",
            "        3548,  1012,  2023,  2453,   103,  2242,  6517,  2055,   103,\n",
            "        6537,   103, 26452,   103,  1999,  2119,  8607,  1024,  1996,\n",
            "        5501,  2453,  2514,  2008,   103,  8568,  1996, 20398,  1997,\n",
            "         103,  2717,  1997,  1996,   103, 22991,  2069,  2729,  2055,\n",
            "         103,   103, 19817,   103, 11209,  1010,   103,  2027,   103,\n",
            "        2079,  1996,  2168,  2007,  2037,  2460,  3152,  1012,   103,\n",
            "        7987,  1013,  1028,  1026,  7987,  1013,  1028, 11341,   103,\n",
            "        1010,   103,  2179,  5977,  9502,  1005,  1055,  3538,  2028,\n",
            "         103,  1996,   103,   103,  1999,  1996,  3074,  1010,  1998,\n",
            "        1008,  1008,   103, 27594,  2121,  3805,  1008,  1008,  1008,\n",
            "        1045,  2036,  3984,  2010, 13954,  1997,  8471,   103,   103,\n",
            "        2004,  1037,  2431,  1011,  4689,   103,  2158,   103, 18150,\n",
            "        5844,   103,  1037,   103,  2259,   103, 13417,   103,  7794,\n",
            "        2166,  1005,  1055,  5292, 10828, 10458,  2617,   103,  1996,\n",
            "        3103, 12342,  2015,  2083,  2010,  3332,  2044,  1996,   103,\n",
            "       13535,   103,  7798,   102], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
            "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>, 'padding_mask': <tf.Tensor: shape=(256,), dtype=bool, numpy=\n",
            "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True])>, 'mask_positions': <tf.Tensor: shape=(64,), dtype=int64, numpy=\n",
            "array([  4,  13,  14,  16,  18,  19,  22,  27,  29,  31,  37,  50,  54,\n",
            "        57,  66,  67,  74,  80,  81,  84,  85, 106, 107, 108, 110, 115,\n",
            "       116, 121, 125, 127, 129, 132, 133, 139, 144, 148, 149, 153, 154,\n",
            "       156, 157, 159, 161, 170, 179, 181, 189, 191, 192, 200, 214, 215,\n",
            "       221, 222, 223, 226, 228, 230, 232, 234, 238, 241, 251, 253])>}, <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
            "array([ 1996,  2015,  1006,  2179,  1996,  6811,  1007,  1010,  2021,\n",
            "        4916,  3471, 12632,  2111,  1010,  8387,  1998,  9231,  2668,\n",
            "       14479,  5920, 20109,  1010,  1998,  1996,  1997,  1997,  4004,\n",
            "        2360,  1996,  1997,  1010,  3971,  1024,  4841,  1996,  2088,\n",
            "        1998,  2037,  2219, 18655,  3111,  2096,  6464,  1026,  2870,\n",
            "        1045,  1997,  2200,  2190,  1008, 28709, 19105,  2214,  2158,\n",
            "        2310,  1999,  2047,  4257,  2010,  2166,  9397,  2043,  1059,\n",
            "        1000], dtype=int32)>, <tf.Tensor: shape=(64,), dtype=float16, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float16)>)\n"
          ]
        }
      ],
      "source": [
        "# All BERT `en` models have the same vocabulary, so reuse preprocessor from\n",
        "# \"bert_tiny_en_uncased\"\n",
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    sequence_length=256,\n",
        ")\n",
        "packer = preprocessor.packer\n",
        "tokenizer = preprocessor.tokenizer\n",
        "\n",
        "# keras.Layer to replace some input tokens with the \"[MASK]\" token\n",
        "masker = keras_nlp.layers.MLMMaskGenerator(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    mask_selection_rate=0.25,\n",
        "    mask_selection_length=64,\n",
        "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
        "    unselectable_token_ids=[\n",
        "        tokenizer.token_to_id(x) for x in [\"[CLS]\", \"[PAD]\", \"[SEP]\"]\n",
        "    ],\n",
        ")\n",
        "\n",
        "def preprocess(inputs, label):\n",
        "    inputs = preprocessor(inputs)\n",
        "    masked_inputs = masker(inputs[\"token_ids\"])\n",
        "    # Split the masking layer outputs into a (features, labels, and weights)\n",
        "    # tuple that we can use with keras.Model.fit().\n",
        "    features = {\n",
        "        \"token_ids\": masked_inputs[\"token_ids\"],\n",
        "        \"segment_ids\": inputs[\"segment_ids\"],\n",
        "        \"padding_mask\": inputs[\"padding_mask\"],\n",
        "        \"mask_positions\": masked_inputs[\"mask_positions\"],\n",
        "    }\n",
        "    labels = masked_inputs[\"mask_ids\"]\n",
        "    weights = masked_inputs[\"mask_weights\"]\n",
        "    return features, labels, weights\n",
        "\n",
        "pretrain_ds = imdb_train.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "pretrain_val_ds = imdb_test.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Tokens with ID 103 are \"masked\"\n",
        "print(pretrain_ds.unbatch().take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2uX0BuWDOdW"
      },
      "source": [
        "## Pretraining model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvJAzL6Gcaci",
        "outputId": "117ea384-6ef0-4813-bfc0-d42fe6dd2344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_12 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_11 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_9 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " bert_backbone_3 (BertBackbone)  {'sequence_output':  4385920    ['input_12[0][0]',               \n",
            "                                 (None, None, 128),               'input_11[0][0]',               \n",
            "                                 'pooled_output': (               'input_10[0][0]',               \n",
            "                                None, 128)}                       'input_9[0][0]']                \n",
            "                                                                                                  \n",
            " mlm_head_2 (MLMHead)           (None, None, 30522)  3954106     ['bert_backbone_3[0][1]',        \n",
            "                                                                  'input_12[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,433,210\n",
            "Trainable params: 4,433,210\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['pooled_dense/kernel:0', 'pooled_dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['pooled_dense/kernel:0', 'pooled_dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 119s 67ms/step - loss: 5.2670 - sparse_categorical_accuracy: 0.0846 - val_loss: 4.9760 - val_sparse_categorical_accuracy: 0.1162\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 93s 60ms/step - loss: 4.9556 - sparse_categorical_accuracy: 0.1244 - val_loss: 4.8710 - val_sparse_categorical_accuracy: 0.1301\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 93s 60ms/step - loss: 4.8342 - sparse_categorical_accuracy: 0.1406 - val_loss: 4.6404 - val_sparse_categorical_accuracy: 0.1709\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdcd66dcac0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# BERT backbone\n",
        "backbone = keras_nlp.models.BertBackbone(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    num_layers=2,\n",
        "    num_heads=2,\n",
        "    hidden_dim=128,\n",
        "    intermediate_dim=512,\n",
        ")\n",
        "\n",
        "# Language modeling head\n",
        "mlm_head = keras_nlp.layers.MLMHead(\n",
        "    embedding_weights=backbone.token_embedding.embeddings,\n",
        ")\n",
        "\n",
        "inputs = {\n",
        "    \"token_ids\": keras.Input(shape=(None,), dtype=tf.int32),\n",
        "    \"segment_ids\": keras.Input(shape=(None,), dtype=tf.int32),\n",
        "    \"padding_mask\": keras.Input(shape=(None,), dtype=tf.int32),\n",
        "    \"mask_positions\": keras.Input(shape=(None,), dtype=tf.int32),\n",
        "}\n",
        "\n",
        "# Encoded token sequence\n",
        "sequence = backbone(inputs)[\"sequence_output\"]\n",
        "\n",
        "# Predict an output word for each masked input token.\n",
        "# We use the input token embedding to project from our encoded vectors to\n",
        "# vocabulary logits, which has been shown to improve training efficiency.\n",
        "outputs = mlm_head(sequence, mask_positions=inputs[\"mask_positions\"])\n",
        "\n",
        "# Define and compile our pretraining model.\n",
        "pretraining_model = keras.Model(inputs, outputs)\n",
        "pretraining_model.summary()\n",
        "pretraining_model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.experimental.AdamW(learning_rate=5e-4),\n",
        "    weighted_metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "\n",
        "# Pretrain on IMDB dataset\n",
        "pretraining_model.fit(\n",
        "    pretrain_ds,\n",
        "    validation_data=pretrain_val_ds,\n",
        "    epochs=3,    # Increase to 6 for higher accuracy\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQS-VVmLJ2mB"
      },
      "source": [
        "After pretraining save your `backbone` submodel to use in a new task!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkEvwzWgJQmV"
      },
      "source": [
        "# Build and train your own transformer from scratch\n",
        "![picture](https://drive.google.com/uc?id=1pzwLPCtvzmHY3DKzH-MBzmjWFJ3pKVB5)\n",
        "\n",
        "Want to implement a novel transformer architecture? The KerasNLP library offers all the low-level modules used to build SoTA architectures in our `models` API. This includes the `keras_nlp.tokenizers` API which allows you to train your own subword tokenizer using `WordPieceTokenizer`, `BytePairTokenizer`, or `SentencePieceTokenizer`.\n",
        "\n",
        "In this workflow we train a custom tokenizer on the IMDB data and design a backbone with custom transformer architecture. For simplicity we then train directly on the classification task. Interested in more details? We wrote an entire guide to pretraining and finetuning a custom transformer: https://keras.io/guides/keras_nlp/transformer_pretraining/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCFbQp2SEHcE"
      },
      "source": [
        "## Train custom vocabulary from IMBD data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOLm5IwHO3as"
      },
      "outputs": [],
      "source": [
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    imdb_train.map(lambda x, y: x),\n",
        "    vocabulary_size=20_000,\n",
        "    lowercase=True,\n",
        "    strip_accents=True,\n",
        "    reserved_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[MASK]\", \"[UNK]\"],\n",
        ")\n",
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    lowercase=True,\n",
        "    strip_accents=True,\n",
        "    oov_token=\"[UNK]\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR9BijJ3EQWq"
      },
      "source": [
        "\n",
        "## Preprocess data with custom tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaxnvAzGe-oe",
        "outputId": "472e3b4c-5ff8-419e-e94c-829c69ef13c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
            "array([    1,    96,   245,  4520,  9667,   100,   144,    98,    43,\n",
            "        3590,    99,    96,  2201,    17,   392, 17025,    98,    96,\n",
            "          11,  5362,   165,   192, 11408,  6869,  7655,    98,    96,\n",
            "         535,  1636,    12,  1604,   986, 18649,  6623,  9654,    13,\n",
            "          18,  1486,   127,    51,   134,  1146,    16,   131,    11,\n",
            "          61,   174,  7595,   133,  3241,  2217,    98,  7668,   775,\n",
            "        6263,  4281,    16,  2056,   136,  1625,   373,    16,   978,\n",
            "         136, 11070,  1720,  3267,    16,  1048,     6,  3256, 15500,\n",
            "       14959,     6,  4942,    16,    97,  2443,  6453,    18,   104,\n",
            "         737, 12968,  1314,  1169,   129,  4226, 12968,    97,    60,\n",
            "        7150, 15515,   100,   237,   100,   147,   544, 16477,   109,\n",
            "         499,  8007,   122,   281,    18,   124,   122,   578,   128,\n",
            "         166,   130,    96,   238,   244,   105,   416,    98,   163,\n",
            "        1405,   124,   189,   961,   120,    18,   138,   115,    16,\n",
            "         124,    11,   246,   533,   147,  1130,  7063, 11464,   136,\n",
            "          18,   151,    96,  3073,    16,  7322,  1336, 18649,  3876,\n",
            "         306,   126,  1229, 15466,   219,   146,   116,   306,  4723,\n",
            "         125,    43,  5157,   107,     6,  1171,   105,  5157,     6,\n",
            "         394,   114,    96,   696,    18,   640,  1285,   108,   550,\n",
            "          98,    11,  5362,   840,    97,  4327,   317,    18,     2,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0],\n",
            "      dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ]
        }
      ],
      "source": [
        "packer = keras_nlp.layers.StartEndPacker(\n",
        "    start_value=tokenizer.token_to_id(\"[START]\"),\n",
        "    end_value=tokenizer.token_to_id(\"[END]\"),\n",
        "    pad_value=tokenizer.token_to_id(\"[PAD]\"),\n",
        "    sequence_length=512,\n",
        ")\n",
        "\n",
        "def preprocess(x, y):\n",
        "    token_ids = packer(tokenizer(x))\n",
        "    return token_ids, y\n",
        "\n",
        "imdb_preproc_train_ds = imdb_train.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "imdb_preproc_val_ds = imdb_test.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(imdb_preproc_train_ds.unbatch().take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QoG4q7PEVdE"
      },
      "source": [
        "\n",
        "## Design a tiny transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aL1FpB16v_I",
        "outputId": "411de1ae-22f0-4ed0-adab-5d1bb3c2baba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " token_ids (InputLayer)      [(None, None)]            0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, None, 64)         1259648   \n",
            " g_1 (TokenAndPositionEmbedd                                     \n",
            " ing)                                                            \n",
            "                                                                 \n",
            " transformer_encoder_3 (Tran  (None, None, 64)         33472     \n",
            " sformerEncoder)                                                 \n",
            "                                                                 \n",
            " tf.__operators__.getitem_7   (None, 64)               0         \n",
            " (SlicingOpLambda)                                               \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,293,250\n",
            "Trainable params: 1,293,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "token_id_input = keras.Input(\n",
        "    shape=(None,), dtype=\"int32\", name=\"token_ids\",\n",
        ")\n",
        "outputs = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=len(vocab),\n",
        "    sequence_length=packer.sequence_length,\n",
        "    embedding_dim=64,\n",
        ")(token_id_input)\n",
        "outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    num_heads=2,\n",
        "    intermediate_dim=128,\n",
        "    dropout=0.1,\n",
        ")(outputs)\n",
        "# Use \"[START]\" token to classify\n",
        "outputs = keras.layers.Dense(2)(outputs[:, 0, :])\n",
        "model = keras.Model(\n",
        "    inputs=token_id_input,\n",
        "    outputs=outputs,\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4rPA8IYE9cJ"
      },
      "source": [
        "## Train the transformer directly on the classification objective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNnaMPIxvi_9",
        "outputId": "cce8da32-c095-437e-b192-4b7cf4e4c558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 129s 78ms/step - loss: 0.6229 - sparse_categorical_accuracy: 0.6260 - val_loss: 0.4061 - val_sparse_categorical_accuracy: 0.8246\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 120s 76ms/step - loss: 0.3126 - sparse_categorical_accuracy: 0.8726 - val_loss: 0.3339 - val_sparse_categorical_accuracy: 0.8629\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.2446 - sparse_categorical_accuracy: 0.9053 - val_loss: 0.3147 - val_sparse_categorical_accuracy: 0.8730\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff0caf9c550>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.experimental.AdamW(5e-5),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "model.fit(\n",
        "    imdb_preproc_train_ds,\n",
        "    validation_data=imdb_preproc_val_ds,\n",
        "    epochs=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjNA5CSqbzXY"
      },
      "source": [
        "Excitingly, our custom classifier is similar to the performance of fine-tuning `\"bert_tiny_en_uncased\"`! To see the advantages of pretraining and exceed 90% accuracy we would need to use larger **presets** such as `\"bert_base_en_uncased\"`."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}