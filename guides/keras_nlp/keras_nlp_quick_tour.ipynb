{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbischof/keras-io/blob/quickstart/guides/keras_nlp/keras_nlp_quick_tour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install 0.4 preview from source\n",
        "!pip install -q git+https://github.com/keras-team/keras-nlp.git tensorflow==2.10 --upgrade"
      ],
      "metadata": {
        "id": "2h__HbR944VZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0f151e-a1fd-48f1-d7ef-ddaced719fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 578.1 MB 6.6 kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 51.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 52.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 438 kB 67.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 61.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 55.3 MB/s \n",
            "\u001b[?25h  Building wheel for keras-nlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlmPYqUn7YJm"
      },
      "outputs": [],
      "source": [
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Use mixed precision for optimal performance\n",
        "keras.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# KerasNLP: Modular NLP Workflows for Keras\n",
        "\n",
        "\n",
        "`keras-nlp` is a natural language processing library that supports users through their entire development cycle. Our workflows are built from modular components that have SoTA preset weights and architectures when used out-of-the-box and are easily customizable when more control is needed.\n",
        "\n",
        "This library is an extension of the core `keras` API; all high level modules are `Layers` or `Models`. If you are familiar with `keras`, congratulations! You already understand most of `keras-nlp`.\n",
        "\n",
        "This guide demonstrates our modular approach using a sentiment analysis example at six levels of complexity:\n",
        "* Inference with a pretrained classifier\n",
        "* Fine tuning a pretrained backbone\n",
        "* Fine tuning with user-controlled preprocessing\n",
        "* Fine tuning a custom model\n",
        "* Pretraining a backbone model\n",
        "* Build and train your own transformer from scratch\n",
        "\n",
        "Throughout our guide we use Professor Keras, the official Keras mascot, as a visual reference for the complexity of the material:\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1d14Qpmfgjf6zu4z30HBaonH8PYDHgVoU)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API quickstart\n",
        "\n",
        "Our highest level API is `keras_nlp.models`. For each `XX` architecture (e.g., `Bert`), we offer the following modules:\n",
        "* **Tokenizer**: `keras_nlp.models.XXTokenizer`\n",
        "    * Maps raw text to `tf.RaggedTensor`s of token ids.\n",
        "    * Inherits from `keras.Layer`.\n",
        "* **Preprocessor**: `keras_nlp.models.XXPreprocessor`\n",
        "    * Maps raw text to a dictonary of dense tensors consumed by the model.\n",
        "    * Has a `XXTokenizer`.\n",
        "    * Inherits from `keras.Layer`.\n",
        "* **Backbone**: `keras_nlp.models.XXBackbone`\n",
        "    * Maps preprocessed tensors to dense representation. *Does not handle raw text*.\n",
        "    * Inherits from `keras.Model`.\n",
        "* **Task**: e.g., `keras_nlp.models.XXClassifier`\n",
        "    * Maps raw text to task-specific output (e.g., classification probabilities).\n",
        "    * Has a `XXBackbone` and `XXPreprocessor`.\n",
        "    * Inherits from `keras.Model`.\n",
        "\n",
        "Here is the modular hierarchy for `BertClassifier` (all relationships are compositional):\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1vHBQ1oFbto8ItfhsLcxKhIwOIdJE1X9n)\n",
        "\n",
        "All modules can be used independently and have a `from_preset()` method in addition to the standard constructor that instantiates the class with **preset** architecture and weights (see examples below)."
      ],
      "metadata": {
        "id": "G7aT5DHgGCvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n",
        "We will use a running example of sentiment analysis of IMDB movie reviews. In this task, we use the text to predict whether the review was positive (`label = 1`) or negative (`label = 0`).\n",
        "\n",
        "We load the data from `tensorflow_datasets`, a collection of machine learning benchmarks that uses the powerful `tf.data.Dataset` format for examples."
      ],
      "metadata": {
        "id": "zlyeJqptmagk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "imdb_train, imdb_test = tfds.load(\n",
        "    \"imdb_reviews\",\n",
        "    split=[\"train\", \"test\"],\n",
        "    as_supervised=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# Inspect first review\n",
        "# Format is (review text tensor, label tensor)\n",
        "imdb_train.unbatch().take(1).get_single_element()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3Q1wpT6mh5o",
        "outputId": "343141c2-af1f-43c0-814f-c913f8043454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">,\n",
              " <tf.Tensor: shape=(), dtype=int64, numpy=0>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference with a pretrained classifier\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1xeMHVCxYhm3_oC37Gg7k0bG-yhsVr0Dv)\n",
        "\n",
        "The highest level module in `keras-nlp` is a **task**. A **task** is a `keras.Model` consisting of a (generally pretrained) **backbone** model and task-specific layers. Here's an example using `keras_nlp.models.BertClassifier`.\n",
        "\n",
        "**Note**: Outputs are the logits per class (`[0, 0]` is 50% chance of positive).\n",
        "\n"
      ],
      "metadata": {
        "id": "yaecCDgL0OfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n",
        "# Note: batched inputs expected so must wrap string in iterable\n",
        "classifier.predict([\"I love modular workflows in keras-nlp!\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdckAMP_6nkH",
        "outputId": "24d7cf21-918d-4276-f6ef-8de2dcabe7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_tiny_en_uncased_sst2/vocab.txt\n",
            "231508/231508 [==============================] - 0s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_tiny_en_uncased_sst2/model.h5\n",
            "17596448/17596448 [==============================] - 1s 0us/step\n",
            "1/1 [==============================] - 4s 4s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.54 ,  1.544]], dtype=float16)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5pVOIqsBF_OB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All **tasks** have a `from_preset` method that constructs a `keras.Model` instance with preset preprocessing, architecture and weights. This means that we can pass raw strings in any format accepted by a `keras.Model` and get output specific to our task.\n",
        "\n",
        "This particular **preset** is a `bert_tiny_uncased_en` **backbone** fine-tuned on `sst2`, another movie review sentiment analysis (this time from Rotten Tomatoes). We use the `tiny` architecture for demo purposes, but larger models are recommended for SoTA performance. For all the task-specific presets available for `BertClassifier`, see [keras.io](https://resilient-dango-43f7b8.netlify.app/api/keras_nlp/models/).\n",
        "\n",
        "Let's evaluate our classifier on the IMDB dataset. We first need to compile the `keras.Model`. Since we are not training, we do not need a `loss` argument."
      ],
      "metadata": {
        "id": "T7sz3bVw83tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.compile(\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        "    jit_compile=True,\n",
        ")\n",
        "\n",
        "classifier.evaluate(imdb_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD-NOMU-CG_8",
        "outputId": "42844384-bc6b-41ab-aafe-6eff4fe45a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 91s 56ms/step - loss: 0.0000e+00 - sparse_categorical_accuracy: 0.7836\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0, 0.7835599780082703]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning a pretrained BERT backbone\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1YytOYRSqsrhJ4NLatVOSuVMbLPa9iXrw)\n",
        "\n",
        "When labeled text specific to our task is available, fine-tuning a custom classifier can improve performance. If we want to predict IMDB review sentiment, using IMDB data should perform better than Rotten Tomatoes data! And for many tasks no relevant pretrained model will be available (e.g., categorizing customer reviews).\n",
        "\n",
        "The workflow for fine-tuning is almost identical to above, except that we request a **preset** for the **backbone**-only model rather than the entire classifier. When passed a **backone** **preset**, a **task** `Model` will randomly initialize all task-specific layers in preparation for training. For all the **backbone** presets available for `BertClassifier`, see [keras.io](https://resilient-dango-43f7b8.netlify.app/api/keras_nlp/models/).\n",
        "\n",
        "To train your classifier, use `Model.compile()` and `Model.fit()` as with any other `keras.Model`. Since preprocessing is included in all **tasks** by default, we again pass the raw data.\n"
      ],
      "metadata": {
        "id": "gSea_qw0_UH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    num_classes=2,\n",
        ")\n",
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.experimental.AdamW(5e-5),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "classifier.fit(\n",
        "    imdb_train,\n",
        "    validation_data=imdb_test,\n",
        "    epochs=1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHAPoLgL_7Se",
        "outputId": "d02a04ee-ded6-4fc2-f74a-30edcddf8522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 482s 296ms/step - loss: 0.4178 - sparse_categorical_accuracy: 0.8042 - val_loss: 0.3119 - val_sparse_categorical_accuracy: 0.8685\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0481100f10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see significant lift in validation accuracy (0.78 -> 0.87) with a single epoch of training even though the IMDB dataset is much smaller than `sst2`.\n"
      ],
      "metadata": {
        "id": "GRz37_8Gu1gQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning with user-controlled preprocessing\n",
        "![picture](https://drive.google.com/uc?id=1T_40vtl8daihS-kKYTFWejFd19KJAyDK)\n",
        "\n",
        "For some advanced training scenarios, users might prefer direct control over preprocessing. For large datasets, examples can be preprocessed in advance and saved to disk or preprocessed by a separate worker pool using `tf.data.experimental.service`. In other cases, custom preprocessing is needed to handle the inputs.\n",
        "\n",
        "Pass `preprocessor=None` to the constructor of a **task** `Model` to skip automatic preprocessing or supply your own `keras.Layer` to perform a custom operation instead.\n",
        "\n"
      ],
      "metadata": {
        "id": "fgTttBkcCvb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separate preprocessing from the same preset\n",
        "\n",
        "Each model architecture has a parallel **preprocessor** `Layer` with its own `from_preset` constructor. Using the same **preset** for this `Layer` will return the matching **preprocessor** as the **task**.\n",
        "\n",
        "In this workflow we train the model over three epochs using `tf.data.Dataset.cache()`, which computes the preprocessing once and caches the result before fitting begins.\n",
        "\n",
        "**Note:** this code only works if your data fits in memory. If not, pass a `filename` to `cache()`."
      ],
      "metadata": {
        "id": "Q_fkpErSkyCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_tiny_en_uncased\"\n",
        ")\n",
        "\n",
        "imdb_train_cached = imdb_train.map(\n",
        "    preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "imdb_test_cached = imdb_test.map(\n",
        "    preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    preprocessor=None,\n",
        ")\n",
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.experimental.AdamW(5e-5),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "classifier.fit(\n",
        "    imdb_train_cached,\n",
        "    validation_data=imdb_test_cached,\n",
        "    epochs=3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYjpZzRSDmmv",
        "outputId": "fbff16d8-b30b-44f9-bcee-58f50f5d0dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 314s 191ms/step - loss: 0.4164 - sparse_categorical_accuracy: 0.8062 - val_loss: 0.3154 - val_sparse_categorical_accuracy: 0.8662\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 240s 153ms/step - loss: 0.2635 - sparse_categorical_accuracy: 0.8917 - val_loss: 0.3058 - val_sparse_categorical_accuracy: 0.8742\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 244s 156ms/step - loss: 0.1923 - sparse_categorical_accuracy: 0.9296 - val_loss: 0.3140 - val_sparse_categorical_accuracy: 0.8786\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e98c76d30>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After three epochs, our validation accuracy has only increased to 0.88. This is mainly a function of the small size of our dataset; even with the `bert_tiny` architecture we've already learned most generalizable patterns in the first pass."
      ],
      "metadata": {
        "id": "6T1qBvAcyhfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom preprocessing\n",
        "\n",
        "In cases where custom preprocessing is required, we offer direct access to the `Tokenizer` class that maps raw strings to tokens. It also has a `from_preset` constructor to get the vocabulary matching pretraining.\n",
        "\n",
        "**Note:** `Tokenizer` does not pad sequences, so output is `tf.RaggedTensor`.\n",
        "\n"
      ],
      "metadata": {
        "id": "aEU576ooLa15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_tiny_en_uncased\")\n",
        "tokenizer([\"I love modular workflows!\", \"Libraries over frameworks!\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8FbsYprLZ7V",
        "outputId": "7c2500b8-60ab-4db5-b7ec-d10522344142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[1045, 2293, 19160, 2147, 12314, 2015, 999],\n",
              " [1045, 2064, 1005, 1056, 3233, 26666, 3642, 1012]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your own packer or use one our `Layers`\n",
        "packer = keras_nlp.layers.MultiSegmentPacker(\n",
        "    start_value=tokenizer.cls_token_id,\n",
        "    end_value=tokenizer.sep_token_id,\n",
        "    sequence_length=64,\n",
        ")\n",
        "\n",
        "def preprocess(x, y):\n",
        "    token_ids, segment_ids = packer(tokenizer(x))\n",
        "    x = {\n",
        "        \"token_ids\": token_ids,\n",
        "        \"segment_ids\": segment_ids,\n",
        "        \"padding_mask\": token_ids != 0,\n",
        "    }\n",
        "    return x, y\n",
        "\n",
        "imbd_train_preprocessed = imdb_train.map(\n",
        "    preprocess, tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "imdb_test_preprocessed = imdb_test.map(\n",
        "    preprocess, tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Preprocessed example\n",
        "imbd_train_preprocessed.unbatch().take(1).get_single_element()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGvck1j-MZ_M",
        "outputId": "3132fd36-7cc6-48e1-e9a7-863094f48342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'token_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
              "  array([  101,  2023,  2001,  2019,  7078,  6659,  3185,  1012,  2123,\n",
              "          1005,  1056,  2022, 26673,  1999,  2011,  5696,  3328,  2368,\n",
              "          2030,  2745,  3707,  7363,  1012,  2119,  2024,  2307,  5889,\n",
              "          1010,  2021,  2023,  2442,  3432,  2022,  2037,  5409,  2535,\n",
              "          1999,  2381,  1012,  2130,  2037,  2307,  3772,  2071,  2025,\n",
              "          2417, 21564,  2023,  3185,  1005,  1055,  9951,  9994,  1012,\n",
              "          2023,  3185,  2003,  2019,  2220,  3157,  7368,  2149, 10398,\n",
              "           102], dtype=int32)>,\n",
              "  'segment_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
              "  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        dtype=int32)>,\n",
              "  'padding_mask': <tf.Tensor: shape=(64,), dtype=bool, numpy=\n",
              "  array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True])>},\n",
              " <tf.Tensor: shape=(), dtype=int64, numpy=0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning with a custom model\n",
        "![picture](https://drive.google.com/uc?id=1T_40vtl8daihS-kKYTFWejFd19KJAyDK)\n",
        "\n",
        "For more advanced applications, an appropriate **task** `Model` may not be available. In this case we provide direct access to the **backbone** `Model`, which has its own `from_preset` constructor and can be composed with custom `Layer`s. Detailed examples can be found at https://keras.io/guides/transfer_learning/.\n",
        "\n",
        "A **backbone** `Model` does not include automatic preprocessing but can be paired with a matching **preprocessor** using the same **preset** as shown in the previous workflow.\n",
        "\n",
        "In this workflow we experiment with freezing our backbone model and adding two trainable transfomer layers to adapt to the new input.\n",
        "\n",
        "**Note**: We can igonore the warning about gradients for the `pooled_dense` layer because we are using BERT's sequence output.\n"
      ],
      "metadata": {
        "id": "7kF05VQIJFMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_tiny_en_uncased\")\n",
        "backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_tiny_en_uncased\")\n",
        "\n",
        "imdb_train_preprocessed = imdb_train.map(\n",
        "    preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "imdb_test_preprocessed = imdb_test.map(\n",
        "    preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "backbone.trainable = False\n",
        "inputs = backbone.input\n",
        "sequence = backbone(inputs)[\"sequence_output\"]\n",
        "for _ in range(2):\n",
        "  sequence = keras_nlp.layers.TransformerEncoder(\n",
        "      num_heads=2,\n",
        "      intermediate_dim=512,\n",
        "      dropout=0.1,\n",
        "  )(sequence, padding_mask=inputs[\"padding_mask\"])\n",
        "# Use [CLS] token output to classify\n",
        "outputs = keras.layers.Dense(2)(sequence[:, backbone.cls_token_index, :])\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.experimental.AdamW(5e-5),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "model.summary()\n",
        "model.fit(\n",
        "    imdb_train_preprocessed,\n",
        "    validation_data=imdb_test_preprocessed,\n",
        "    epochs=3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yivypQjiRdi7",
        "outputId": "29f8c950-d528-4415-ef39-b7bb56a07fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_tiny_en_uncased/v1/model.h5\n",
            "17602216/17602216 [==============================] - 1s 0us/step\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " padding_mask (InputLayer)      [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " segment_ids (InputLayer)       [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " token_ids (InputLayer)         [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " bert_backbone_1 (BertBackbone)  {'sequence_output':  4385920    ['padding_mask[0][0]',           \n",
            "                                 (None, None, 128),               'segment_ids[0][0]',            \n",
            "                                 'pooled_output': (               'token_ids[0][0]']              \n",
            "                                None, 128)}                                                       \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Transfo  (None, None, 128)   198272      ['bert_backbone_1[0][1]',        \n",
            " rmerEncoder)                                                     'padding_mask[0][0]']           \n",
            "                                                                                                  \n",
            " transformer_encoder_2 (Transfo  (None, None, 128)   198272      ['transformer_encoder_1[0][0]',  \n",
            " rmerEncoder)                                                     'padding_mask[0][0]']           \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_3 (Sl  (None, 128)         0           ['transformer_encoder_2[0][0]']  \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2)            258         ['tf.__operators__.getitem_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,782,722\n",
            "Trainable params: 396,802\n",
            "Non-trainable params: 4,385,920\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 127s 69ms/step - loss: 0.5783 - sparse_categorical_accuracy: 0.6976 - val_loss: 0.4604 - val_sparse_categorical_accuracy: 0.7832\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 22s 14ms/step - loss: 0.4631 - sparse_categorical_accuracy: 0.7808 - val_loss: 0.4071 - val_sparse_categorical_accuracy: 0.8141\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 22s 14ms/step - loss: 0.4274 - sparse_categorical_accuracy: 0.8047 - val_loss: 0.3893 - val_sparse_categorical_accuracy: 0.8245\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f042cc6f250>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model achieves reasonable accuracy despite having only 10% the trainable parameters of our `BertClassifier` model. Each training step takes about 1/3 of the time---even accounting for cached preprocessing."
      ],
      "metadata": {
        "id": "fnYHsiQYEDY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretraining a backbone model\n",
        "![picture](https://drive.google.com/uc?id=1pzwLPCtvzmHY3DKzH-MBzmjWFJ3pKVB5)\n",
        "\n",
        "Do you have access to large unlabeled datasets in your domain? Are they are around the same size as used to train popular backbones such as BERT, RoBERTa, or GPT2 (XX+ GiB)? If so, you might benefit from domain-specific pretraining of your own backbone models.\n",
        "\n",
        "NLP models are generally pretrained on a language modeling task, predicting masked words given the visible words in an input sentence. For example, given the input `\"The fox [MASK] over the [MASK] dog\"`, the model might be asked to predict `[\"jumped\", \"lazy\"]`. The lower layers of this model are then packaged as a **backbone** to be combined with layers relating to a new task.\n",
        "\n",
        "The `keras-nlp` library offers SoTA **backbones** and **tokenizers** to be trained from scratch without presets.\n",
        "\n",
        "In this workflow we pretrain a BERT **backbone** using our IMDB review text. We skip the \"next sentence prediction\" (NSP) loss because it adds significant complexity to the data processing and was dropped by later models like RoBERTa. See our e2e [BERT pretraining example](https://github.com/keras-team/keras-nlp/tree/4f9ebefa82af22b4f4267dfa80fa525f7a03bd5d/examples/bert) for step-by-step details on how to replicate the original paper."
      ],
      "metadata": {
        "id": "9HcdM0yuTKpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "tERSi-TnDKOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All BERT `en` models have the same vocabulary, so reuse preprocessor from\n",
        "# \"bert_tiny_en_uncased\"\n",
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "packer = preprocessor.packer\n",
        "tokenizer = preprocessor.tokenizer\n",
        "\n",
        "# keras.Layer to replace some input tokens with the \"[MASK]\" token\n",
        "masker = keras_nlp.layers.MLMMaskGenerator(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    mask_selection_rate=0.25,\n",
        "    mask_selection_length=32,\n",
        "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
        "    unselectable_token_ids=[\n",
        "        tokenizer.token_to_id(x) for x in [\"[CLS]\", \"[PAD]\", \"[SEP]\"]\n",
        "    ],\n",
        ")\n",
        "\n",
        "def preprocess(inputs, label):\n",
        "    inputs = preprocessor(inputs)\n",
        "    masked_inputs = masker(inputs[\"token_ids\"])\n",
        "    # Split the masking layer outputs into a (features, labels, and weights)\n",
        "    # tuple that we can use with keras.Model.fit().\n",
        "    features = {\n",
        "        \"token_ids\": masked_inputs[\"token_ids\"],\n",
        "        \"segment_ids\": inputs[\"segment_ids\"],\n",
        "        \"padding_mask\": inputs[\"padding_mask\"],\n",
        "        \"mask_positions\": masked_inputs[\"mask_positions\"],\n",
        "    }\n",
        "    labels = masked_inputs[\"mask_ids\"]\n",
        "    weights = masked_inputs[\"mask_weights\"]\n",
        "    return features, labels, weights\n",
        "\n",
        "pretrain_ds = imdb_train.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "pretrain_val_ds = imdb_test.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Tokens with ID 103 are \"masked\"\n",
        "pretrain_ds.unbatch().take(1).get_single_element()"
      ],
      "metadata": {
        "id": "Cad9cIrGTIPP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e7f6b64-e715-4c3c-befc-af52dcd2c271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'token_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
              "  array([  101,  2023,  2001,  2019,  7078,  6659,  3185,   103,  2123,\n",
              "          1005,  1056,   103, 26673,  1999,   103,  5696,  3328,  2368,\n",
              "          2030,   103,  3707,  7363,  1012,  2119,   103,  2307,  5889,\n",
              "          1010,  2021,  2023,  2442,   103,   103,  2037,  5409,  2535,\n",
              "          1999,  2381,  1012,  2130,  2037,  2307,  3772,   103,  2025,\n",
              "          2417,   103,  2023,   103,  1005,  1055,  9951,  9994,  1012,\n",
              "           103,   103,  2003,  2019,  2220,  3157,  7368,  2149,   103,\n",
              "          3538,  1012,  1996,  2087, 17203,  5019,   103,  2216,  2043,\n",
              "          1996, 25882,  8431,  2020,   103,  2037,  3572,  2005, 25239,\n",
              "          1012,   103, 21878,   103,  2696,   103,  2596,  6887, 16585,\n",
              "          1010,  1998,  2014, 18404,   103,   103,  6771, 11378,  3328,\n",
              "          2368,   103,   103,  2021,  1037,   103,  6832, 13354,   103,\n",
              "           103,  3185,   103,  2001, 22808,  1997,  2151,  2613, 28940,\n",
              "          1012,  1045,   103,  9364,   103,  2045,  2024,  5691,  2066,\n",
              "           103,   102], dtype=int32)>,\n",
              "  'segment_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
              "  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>,\n",
              "  'padding_mask': <tf.Tensor: shape=(128,), dtype=bool, numpy=\n",
              "  array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True])>,\n",
              "  'mask_positions': <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
              "  array([  7,  11,  14,  19,  24,  31,  32,  43,  46,  48,  54,  55,  62,\n",
              "          69,  76,  82,  83,  84,  86,  94,  95,  97, 100, 101, 104, 107,\n",
              "         108, 110, 116, 119, 121, 126])>},\n",
              " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
              " array([ 1012,  2022,  2011,  2745,  2024,  3432,  2022,  2071, 21564,\n",
              "         3185,  2023,  3185, 10398,  2020,  2437,  3814,  9530,  5428,\n",
              "        17649,  1011,  2293,  2007,  2001,  2498, 17203,  1999,  1037,\n",
              "         2008,  3574,  2572,  2008,  2023], dtype=int32)>,\n",
              " <tf.Tensor: shape=(32,), dtype=float16, numpy=\n",
              " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       dtype=float16)>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretraining model"
      ],
      "metadata": {
        "id": "B2uX0BuWDOdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT backbone\n",
        "backbone = keras_nlp.models.BertBackbone(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    num_layers=2,\n",
        "    num_heads=2,\n",
        "    hidden_dim=128,\n",
        "    intermediate_dim=512,\n",
        ")\n",
        "\n",
        "# Language modeling head\n",
        "mlm_head = keras_nlp.layers.MLMHead(\n",
        "    embedding_weights=backbone.token_embedding.embeddings,\n",
        ")\n",
        "\n",
        "inputs = {\n",
        "    \"token_ids\": keras.Input(shape=(None,), dtype=tf.int32),\n",
        "    \"segment_ids\": keras.Input(shape=(None,), dtype=tf.int32),\n",
        "    \"padding_mask\": keras.Input(shape=(None,), dtype=tf.int32),\n",
        "    \"mask_positions\": keras.Input(shape=(None,), dtype=tf.int32),\n",
        "}\n",
        "\n",
        "# Encoded token sequence\n",
        "sequence = backbone(inputs)[\"sequence_output\"]\n",
        "\n",
        "# Predict an output word for each masked input token.\n",
        "# We use the input token embedding to project from our encoded vectors to\n",
        "# vocabulary logits, which has been shown to improve training efficiency.\n",
        "outputs = mlm_head(sequence, mask_positions=inputs[\"mask_positions\"])\n",
        "\n",
        "# Define and compile our pretraining model.\n",
        "pretraining_model = keras.Model(inputs, outputs)\n",
        "pretraining_model.summary()\n",
        "pretraining_model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.experimental.AdamW(learning_rate=5e-4),\n",
        "    weighted_metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "\n",
        "# Pretrain on IMDB dataset\n",
        "pretraining_model.fit(\n",
        "    pretrain_ds,\n",
        "    validation_data=pretrain_val_ds,\n",
        "    epochs=3,    # Increase to 6 for higher accuracy\n",
        ")"
      ],
      "metadata": {
        "id": "uvJAzL6Gcaci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd17ac63-c9dc-4d31-d44a-69797793fc4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_52 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_51 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_50 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_49 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " bert_backbone_14 (BertBackbone  {'sequence_output':  4385920    ['input_52[0][0]',               \n",
            " )                               (None, None, 128),               'input_51[0][0]',               \n",
            "                                 'pooled_output': (               'input_50[0][0]',               \n",
            "                                None, 128)}                       'input_49[0][0]']               \n",
            "                                                                                                  \n",
            " mlm_head_12 (MLMHead)          (None, None, 30522)  3954106     ['bert_backbone_14[0][1]',       \n",
            "                                                                  'input_52[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,433,210\n",
            "Trainable params: 4,433,210\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['pooled_dense/kernel:0', 'pooled_dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['pooled_dense/kernel:0', 'pooled_dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 88s 47ms/step - loss: 6.2932 - sparse_categorical_accuracy: 0.0827 - val_loss: 5.9596 - val_sparse_categorical_accuracy: 0.1138\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 65s 41ms/step - loss: 5.8553 - sparse_categorical_accuracy: 0.1305 - val_loss: 5.6525 - val_sparse_categorical_accuracy: 0.1663\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 5.5111 - sparse_categorical_accuracy: 0.1848 - val_loss: 5.2346 - val_sparse_categorical_accuracy: 0.2221\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0402dc9610>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After pretraining save your `backbone` submodel to use in a new task!"
      ],
      "metadata": {
        "id": "kQS-VVmLJ2mB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and train your own transformer from scratch\n",
        "![picture](https://drive.google.com/uc?id=1pzwLPCtvzmHY3DKzH-MBzmjWFJ3pKVB5)\n",
        "\n",
        "Want to implement a novel transformer architecture? The `keras-nlp` library offers all the low-level modules used to build SoTA architectures in our `models` API. This includes training your own subword tokenizer using `WordPiece`, `BytePairEncoder`, or `SentencePiece`.\n",
        "\n",
        "In this workflow we train a custom tokenizer on the IMDB data and design a backbone with custom transformer architecture. For simplicity we then train directly on the classification task. Interested in more details? We wrote an entire guide to pretraining and finetuning a custom transformer: https://keras.io/guides/keras_nlp/transformer_pretraining/"
      ],
      "metadata": {
        "id": "wkEvwzWgJQmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train custom vocabulary from IMBD data"
      ],
      "metadata": {
        "id": "GCFbQp2SEHcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    imdb_train.map(lambda x, y: x),\n",
        "    vocabulary_size=10_000,    # Increase to 20_000 for better performance\n",
        "    lowercase=True,\n",
        "    strip_accents=True,\n",
        "    reserved_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[MASK]\", \"[UNK]\"],\n",
        ")\n",
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    lowercase=True,\n",
        "    strip_accents=True,\n",
        "    oov_token=\"[UNK]\",\n",
        ")"
      ],
      "metadata": {
        "id": "EOLm5IwHO3as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Preprocess data with custom tokenizer"
      ],
      "metadata": {
        "id": "AR9BijJ3EQWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "packer = keras_nlp.layers.StartEndPacker(\n",
        "    start_value=tokenizer.token_to_id(\"[START]\"),\n",
        "    end_value=tokenizer.token_to_id(\"[END]\"),\n",
        "    pad_value=tokenizer.token_to_id(\"[PAD]\"),\n",
        "    sequence_length=64,\n",
        ")\n",
        "\n",
        "def preprocess(x, y):\n",
        "    token_ids = packer(tokenizer(x))\n",
        "    x = {\n",
        "        \"token_ids\": token_ids,\n",
        "        \"padding_mask\": token_ids != tokenizer.token_to_id(\"[PAD]\"),\n",
        "    }\n",
        "    return x, y\n",
        "\n",
        "imdb_preproc_train_ds = imdb_train.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "imdb_preproc_val_ds = imdb_test.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "imdb_preproc_train_ds.unbatch().take(1).get_single_element()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaxnvAzGe-oe",
        "outputId": "a0168de4-a430-4a41-b279-96511c17125d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'token_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
              "  array([   1,  104,  106,  127,  539,  500,  110,   18,  183,   11,   62,\n",
              "          121,   54, 3451,  103,  126, 1557, 3771,  134,  585, 5279, 4599,\n",
              "           18,  300,  118,  179,  254,   16,  111,  104,  309,  437,  121,\n",
              "          159,  351,  317,  103,  584,   18,  151,  159,  179,  210,  192,\n",
              "          116, 6815,  104,  110,   11,   61,  772,  903,   18,  104,  110,\n",
              "          100,  127,  504, 3425, 1749,  280, 2828,  524,    2], dtype=int32)>,\n",
              "  'padding_mask': <tf.Tensor: shape=(64,), dtype=bool, numpy=\n",
              "  array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True])>},\n",
              " <tf.Tensor: shape=(), dtype=int64, numpy=0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Design a tiny transformer"
      ],
      "metadata": {
        "id": "2QoG4q7PEVdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_id_input = keras.Input(\n",
        "    shape=(None,), dtype=\"int32\", name=\"token_ids\",\n",
        ")\n",
        "padding_mask = keras.Input(\n",
        "    shape=(None,), dtype=\"int32\", name=\"padding_mask\",\n",
        ")\n",
        "outputs = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=len(vocab),\n",
        "    sequence_length=packer.sequence_length,\n",
        "    embedding_dim=64,\n",
        ")(token_id_input)\n",
        "outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    num_heads=2,\n",
        "    intermediate_dim=128,\n",
        "    dropout=0.1,\n",
        ")(outputs, padding_mask=padding_mask)\n",
        "# Use \"[START]\" token to classify\n",
        "outputs = keras.layers.Dense(2)(outputs[:, 0, :])\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        \"token_ids\": token_id_input,\n",
        "        \"padding_mask\": padding_mask,\n",
        "    },\n",
        "    outputs=outputs,\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aL1FpB16v_I",
        "outputId": "9de3872e-1839-4da0-94f6-78c4268c0e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_22\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " token_ids (InputLayer)         [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " token_and_position_embedding_6  (None, None, 64)    637248      ['token_ids[0][0]']              \n",
            "  (TokenAndPositionEmbedding)                                                                     \n",
            "                                                                                                  \n",
            " padding_mask (InputLayer)      [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_encoder_8 (Transfo  (None, None, 64)    33472       ['token_and_position_embedding_6[\n",
            " rmerEncoder)                                                    0][0]',                          \n",
            "                                                                  'padding_mask[0][0]']           \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_20 (S  (None, 64)          0           ['transformer_encoder_8[0][0]']  \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 2)            130         ['tf.__operators__.getitem_20[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 670,850\n",
            "Trainable params: 670,850\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the transformer directly on the classification objective"
      ],
      "metadata": {
        "id": "F4rPA8IYE9cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.experimental.AdamW(5e-5),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "model.fit(\n",
        "    imdb_preproc_train_ds,\n",
        "    validation_data=imdb_preproc_val_ds,\n",
        "    epochs=3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNnaMPIxvi_9",
        "outputId": "38ef3a94-d32c-4d9d-8259-defde6d7f10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 51s 26ms/step - loss: 0.6882 - sparse_categorical_accuracy: 0.5505 - val_loss: 0.6123 - val_sparse_categorical_accuracy: 0.6823\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5078 - sparse_categorical_accuracy: 0.7490 - val_loss: 0.4961 - val_sparse_categorical_accuracy: 0.7515\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4271 - sparse_categorical_accuracy: 0.7998 - val_loss: 0.4861 - val_sparse_categorical_accuracy: 0.7601\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f03e333bd90>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While our classification accuracy is a fairly poor 0.76, the transformer architecture is too complicated to learn from scratch on a small dataset. The large performance gap with our earlier models shows the power of pretraining and transfer learning in modern NLP."
      ],
      "metadata": {
        "id": "68NEPLv2r8I4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}